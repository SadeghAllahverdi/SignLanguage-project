{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70268d57-0b20-4ff0-a9df-59df88b21d13",
   "metadata": {},
   "source": [
    "# importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956a85df-9df3-4af3-a545-f7382b2b47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random                                                                   # for handelling the files\n",
    "import numpy as np                                                                  # working with arrays and data manupilation\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt                                                     # plotting  \n",
    "import seaborn as sns                                                               # another data visualizor(improves matp)\n",
    "\n",
    "from glob import glob                                                               # for handelling file paths\n",
    "from pathlib import Path                                                            # for handelling file paths\n",
    "from natsort import natsorted                                                       # for sorting files\n",
    "from tqdm.auto import tqdm                                                          # for asthetic for loops :)\n",
    "from collections import Counter                                                     # drawing charts\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split                                # splitting the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65599b28-a505-4b86-84ed-db485c368216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision, torchmetrics, torchinfo\n",
    "import torch.optim as optim                                                         # optimizer\n",
    "import torch.nn.functional as F                                                     # loss function\n",
    "\n",
    "from torch import nn                                                                # building neural networks\n",
    "from torch.utils.data import Dataset                                                # building dataset objects\n",
    "from torch.utils.data import DataLoader                                             # building data loaders to feed the data to model\n",
    "from torch.utils.tensorboard import SummaryWriter                                   # for tracking model performance(easy to use)\n",
    "from torchvision import transforms                                                  # for changes to the dataset content\n",
    "from torchinfo import summary                                                       # gives a summery of model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6db1b8-ed97-4a46-9ff8-e0f7c90f1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2                                                                          # for opening the video files\n",
    "import mediapipe as mp                                                              # for detecting landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27eb68-ddd6-46af-a556-035b5d6e55c0",
   "metadata": {},
   "source": [
    "check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf73b1c3-8f58-4aa0-a673-ba623f2161db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available.\n",
      "Current Device: 0, Device Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is available.\")\n",
    "    print(f\"Current Device: {torch.cuda.current_device()}, Device Name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181142db-0671-4100-baf8-41cfd6df97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "holistic_model= mp.solutions.holistic\n",
    "draw_utils= mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2acd1-803b-4372-94c9-78477e0231e4",
   "metadata": {},
   "source": [
    "I decided to use **LSA64: A Dataset for Argentinian Sign Language**:<br>\r\n",
    "**The sign database** for the Argentinian Sign Language, created with the goal of producing a dictionary for LSA and training an automatic sign recognizer, **includes 3200 videos where 10 non-expert subjects executed 5 repetitions of 64 different types of signs**. Signs were selected among the most commonly used ones in the LSA lexicon, including both verbs and nouns.<br>\n",
    "\n",
    "- I tried to capture detections for the entire dataset but it took around 3 hours. and at the end i had to change the code because i forgot to add a list that would represent the essence of the video in terms of being a sequence of detected frame landmarks.\n",
    "\n",
    "- As seen in the picture below a good number of video samples involve using right hand. after my first experiment as a concept I decided to eliminate detections for left hands because i was only using the first 10 words. and leaving left hand detections created a lot of zeros which i believe were effecting my lstm perfomance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43404a-25e9-4be5-afd3-6acf67aec2ba",
   "metadata": {},
   "source": [
    "![Image Description](lsa64_raw/Images/datasetgeneralinfo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d559b4-a870-4dc8-a4fd-d5066e825323",
   "metadata": {},
   "outputs": [],
   "source": [
    "root= 'lsa64_raw/all'\n",
    "save_directory= 'lsa64_raw/all_results/'\n",
    "class_names= ['Opaque', 'Red', 'Green', 'Yellow', 'Bright', 'Light-blue', 'Colors', 'Pink',\n",
    "              'Women', 'Enemy', 'Son', 'Man', 'Away', 'Drawer', 'Born', 'Learn',\n",
    "              'Call', 'Skimmer', 'Bitter', 'Sweet milk', 'Milk', 'Water', 'Food', 'Argentina',\n",
    "              'Uruguay', 'Country', 'Last name', 'Where', 'Mock', 'Birthday', 'Breakfast', 'Photo',\n",
    "              'Hungry', 'Map', 'Coin', 'Music', 'Ship', 'None', 'Name', 'Patience',\n",
    "              'Perfume', 'Deaf', 'Trap', 'Rice', 'Barbecue', 'Candy', 'Chewing-gum', 'Spaghetti',\n",
    "              'Yogurt', 'Accept', 'Thanks', 'Shut down', 'Appear', 'To land', 'Catch', 'Help',\n",
    "              'Dance', 'Bathe', 'Buy', 'Copy', 'Run', 'Realize', 'Give', 'Find']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987720f0-7849-4b9a-9a08-623b08949aad",
   "metadata": {},
   "source": [
    "# Function to extract detections from all videos in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e4a590-a63e-4e34-b278-c77d0495d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(root: str, class_names, frame_numbers):\n",
    "    \n",
    "    all_video_paths= natsorted([str(p) for p in Path(root).glob(\"**/*.mp4\")])\n",
    "    vid_idx_to_class_name= {i+1:label for i, label in enumerate(class_names)}\n",
    "    none_cv2_video_paths= []\n",
    "    detections= []\n",
    "    labels= []\n",
    "    frame_numbers= frame_numbers\n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for video_path in tqdm(all_video_paths, desc=\"Processing videos\"):\n",
    "            cap = cv2.VideoCapture(video_path)              # Read each video using cv2\n",
    "            if not cap.isOpened():                          # if cv2 can't read the video\n",
    "                none_cv2_video_paths.append(video_path)     # save the video path\n",
    "            else:                                           # if cap can read the video\n",
    "                total_frames_number = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))                                # getting the total frames in video\n",
    "                frame_idxs_to_process = np.linspace(0, total_frames_number-1, frame_numbers, dtype=int)     # picking desiered frame indexes\n",
    "                video_detections= []\n",
    "                for idx in frame_idxs_to_process:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                    ret, frame= cap.read()\n",
    "            \n",
    "                    result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                    pose= np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4) \n",
    "                    face= np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3) \n",
    "                    lh= np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "                    rh= np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "                    detection= np.concatenate((pose,face,lh, rh))\n",
    "                    label= vid_idx_to_class_name[int(os.path.basename(video_path).split('_')[0])]\n",
    "                    video_detections.append(detection)\n",
    "\n",
    "                \n",
    "                detections.append(video_detections)    \n",
    "                label= vid_idx_to_class_name[int(os.path.basename(video_path).split('_')[0])]\n",
    "                labels.append(label)\n",
    "   \n",
    "            cap.release()\n",
    "        \n",
    "    return detections, labels, len(all_video_paths),len(none_cv2_video_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f33b6-6699-4b51-af68-9dab37ebd5f8",
   "metadata": {},
   "source": [
    "# Function to train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862aee0b-145e-449f-867b-33fa0d4a1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, train_dataloader, test_dataloader, optimizer, criterion, calculate_accuracy, device):\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies= [], [], [], []\n",
    " \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Epoch\"):\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0.0, 0.0\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += calculate_accuracy(outputs, labels)\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        train_accuracies.append(train_accuracy / len(train_dataloader))\n",
    "    \n",
    "        model.eval()\n",
    "        all_preds, all_labels= [], []\n",
    "        test_loss, test_accuracy= 0.0, 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                test_accuracy += calculate_accuracy(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)                  # Get the predicted labels\n",
    "                all_preds.extend(preds.view(-1).cpu().numpy())    # Store predictions\n",
    "                all_labels.extend(labels.view(-1).cpu().numpy())  # Store true labels\n",
    "                \n",
    "        \n",
    "        test_losses.append(test_loss / len(test_dataloader))\n",
    "        test_accuracies.append(test_accuracy / len(test_dataloader))\n",
    "        #print(f'Epoch {epoch+1}, Train Loss: {train_losses[-1]}, Test Loss: {test_losses[-1]}, Train Accuracy: {train_accuracies[-1]}, Test Accuracy: {test_accuracies[-1]}')\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7e992-1669-49c1-99c5-ce5e0f00a928",
   "metadata": {},
   "source": [
    "# Function to calculate accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0fff6f3-88e0-4927-8a9a-86cf048727dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    _, predictions = torch.max(y_pred, 1)\n",
    "    correct = (predictions == y_true).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e4be5-9d82-44b6-84f6-144d661270a4",
   "metadata": {},
   "source": [
    "# Function to draw loss and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c541d76a-8baa-4332-87c9-01511a9fc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, batch_size):\n",
    "    train_accuracies_cpu = [x.cpu().item() for x in train_accuracies] \n",
    "    test_accuracies_cpu = [x.cpu().item() for x in test_accuracies]\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')  # or use train_losses_cpu if converted\n",
    "    plt.plot(test_losses, label='Validation Loss')  # or use test_losses_cpu if converted\n",
    "    plt.title(f'Loss over Epochs(batch size= {batch_size})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies_cpu, label='Training Accuracy')\n",
    "    plt.plot(test_accuracies_cpu, label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c790d-50d1-4adb-ae60-8b90114f0b62",
   "metadata": {},
   "source": [
    "# Function to draw confusion matrix of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6efcf28-1dd5-4a15-a66b-b6330071473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confmatrix(all_labels, all_preds, class_names, num_epochs):\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f'Confusion Matrix after {num_epochs} epoches')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c8151a-a878-40be-baf2-133f56be73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result= get_landmarks(root, class_names, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b621e4-9e1e-45d5-9b6f-ab2f90c46cfe",
   "metadata": {},
   "source": [
    "# Saving/Loading detections and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9efbc729-5cb6-4c71-bc64-7952bec1bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(os.path.join(save_directory, 'detections60fps.npy'), np.array(result[0]))     # saving detection \n",
    "#with open(os.path.join(save_directory, 'labels50fps.json'), 'w') as file:\n",
    "    #json.dump(result[1], file)                                                   # saving labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d37ce4-b20d-49eb-9214-178abbe5cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections= np.load(os.path.join(save_directory, 'detections60fps.npy'), allow_pickle=True)\n",
    "with open(os.path.join(save_directory, 'labels50fps.json'), 'r') as file:\n",
    "    labels= json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "803aa7ac-3ba4-485c-b9c0-38058694666a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 60, 1662)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f9684-f8e4-465e-bb6f-ab430fd13bdd",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cddec285-7034-4d03-ac13-25c573b4df57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2560, 60, 1662]),\n",
       " torch.Size([640, 60, 1662]),\n",
       " torch.Size([2560]),\n",
       " torch.Size([640]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map= {label: num for num, label in enumerate(class_names)}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(detections, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train, X_test= torch.tensor(X_train, dtype=torch.float32) , torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train= [label_map[label] for label in y_train]\n",
    "y_test= [label_map[label] for label in y_test]\n",
    "y_train= torch.tensor(y_train, dtype=torch.long)\n",
    "y_test= torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ebd6b-d35a-465e-97f8-20a7f38fb3e4",
   "metadata": {},
   "source": [
    "# Custom class for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ff30d57-ffda-4938-be56-d4c4ed10344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9322042-0c36-4bed-af64-3afb59ad9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= CustomDataset(X_train, y_train)\n",
    "test_dataset= CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9a33928-b33a-4aab-919c-787033fcee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941d61b-ba37-4ccc-8aaf-7a6338489c60",
   "metadata": {},
   "source": [
    "# Class for LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7feb0b2-8f19-4d80-8d2d-fe03a9ed5b54",
   "metadata": {},
   "source": [
    "I built the LSTM model based on a youtube video i saw. in the video the model is made using TensorFlow. Here I implemented the same model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbcc28f3-bb21-442f-bf58-9dc0c9bd361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmModel(nn.Module):\n",
    "    def __init__(self, input_size= 1662, hidden_size= 96, num_classes= 64):\n",
    "        super().__init__()\n",
    "        # Define the LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        #self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        # Define the Dense layers\n",
    "        self.dense1 = nn.Linear(hidden_size, hidden_size)\n",
    "        #self.dense2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # Define activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.lstm1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x, (hn, cn) = self.lstm2(x)\n",
    "        #x = self.relu(x)\n",
    "\n",
    "        x = x[:, -1, :] # here we make x the last prediction of the model beacuse i want to do classification\n",
    "\n",
    "        self.dense1(x)\n",
    "        #x = self.dense2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf4e2af2-9b82-4ab0-b5f1-a587bfd1d8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LstmModel                                [1, 96]                   --\n",
       "├─LSTM: 1-1                              [1, 40, 96]               675,840\n",
       "├─Linear: 1-2                            [1, 96]                   9,312\n",
       "==========================================================================================\n",
       "Total params: 685,152\n",
       "Trainable params: 685,152\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 27.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.27\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 2.74\n",
       "Estimated Total Size (MB): 3.04\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model= LstmModel().to(device)\n",
    "summary(test_model, input_size=(1, 40, 1662)) # summery of the model's structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f2793-41da-4a2d-a950-25c97f1c2158",
   "metadata": {},
   "source": [
    "# Class for transformer model (with positional encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2149883e-131b-4ff4-9abf-e771eb6ea00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelP(nn.Module):\n",
    "    def __init__(self, num_classes= 64, seq_len=40, landmarks=1662, hidden_dim=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.landmarks = landmarks\n",
    "        self.input_proj = nn.Linear(landmarks, hidden_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, seq_len, hidden_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, _ = inputs.shape\n",
    "        inputs_proj = self.input_proj(inputs.view(batch_size * seq_len, -1)).view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Adding positional encodings\n",
    "        inputs_encoded = inputs_proj + self.pos_encoder\n",
    "        \n",
    "        # Passing through the transformer encoder\n",
    "        transformer_output = self.transformer_encoder(inputs_encoded)\n",
    "        \n",
    "        # Using only the output from the last time step for classification\n",
    "        output = self.linear_class(transformer_output[:, -1, :])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d96f7e61-3d2e-46cc-a783-1d0187d28e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerModelP                             [1, 64]                   5,120\n",
       "├─Linear: 1-1                                 [40, 128]                 212,864\n",
       "├─TransformerEncoder: 1-2                     [1, 40, 128]              --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [1, 40, 128]              593,024\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [1, 40, 128]              593,024\n",
       "├─Linear: 1-3                                 [1, 64]                   8,256\n",
       "===============================================================================================\n",
       "Total params: 1,412,288\n",
       "Trainable params: 1,412,288\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 8.52\n",
       "===============================================================================================\n",
       "Input size (MB): 0.27\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.88\n",
       "Estimated Total Size (MB): 1.19\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = TransformerModelP().to(device)\n",
    "summary(test_model, input_size=(1, 40, 1662))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6960e3-5fe8-42c0-bf3c-5e416f837db9",
   "metadata": {},
   "source": [
    "# Class for transformer model (with Conv1D layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d86305e-162a-47d5-a04c-6831dbf3d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelC(nn.Module):\n",
    "    def __init__(self, num_classes=64, seq_len=40, landmarks=1662, hidden_dim=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.landmarks = landmarks\n",
    "        self.input_proj = nn.Conv1d(in_channels=landmarks, out_channels=hidden_dim, kernel_size=1)\n",
    "        # self.pos_encoder = nn.Parameter(torch.randn(1, seq_len, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, _ = inputs.shape\n",
    "        inputs_reshaped = inputs.permute(0, 2, 1)\n",
    "        inputs_proj = self.input_proj(inputs_reshaped)\n",
    "        inputs_proj = inputs_proj.permute(0, 2, 1)\n",
    "        \n",
    "        #inputs_encoded = inputs_proj + self.pos_encoder\n",
    "        \n",
    "        transformer_output = self.transformer_encoder(inputs_proj)\n",
    "        \n",
    "        output = self.linear_class(transformer_output[:, -1, :])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40029ce5-3a13-4151-8c6a-67bf0ef41948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerModelC                             [1, 64]                   --\n",
       "├─Conv1d: 1-1                                 [1, 128, 40]              212,864\n",
       "├─TransformerEncoder: 1-2                     [1, 40, 128]              --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [1, 40, 128]              593,024\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [1, 40, 128]              593,024\n",
       "├─Linear: 1-3                                 [1, 64]                   8,256\n",
       "===============================================================================================\n",
       "Total params: 1,407,168\n",
       "Trainable params: 1,407,168\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 8.52\n",
       "===============================================================================================\n",
       "Input size (MB): 0.27\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.88\n",
       "Estimated Total Size (MB): 1.19\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = TransformerModelC().to(device)\n",
    "summary(test_model, input_size=(1, 40, 1662))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f9d33-e891-4020-a202-2501b4792afe",
   "metadata": {},
   "source": [
    "# Class for transformer model (No positional encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68769523-6b5a-4316-8b3f-3d2dbbc999de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_classes=64, seq_len=40, landmarks=1662, nhead=6, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.landmarks = landmarks\n",
    "        #self.input_proj = nn.Conv1d(in_channels=landmarks, out_channels=hidden_dim, kernel_size=1)\n",
    "        # self.pos_encoder = nn.Parameter(torch.randn(1, seq_len, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=landmarks, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.linear_class = nn.Linear(landmarks, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #batch_size, seq_len, _ = inputs.shape\n",
    "        #inputs_reshaped = inputs.permute(0, 2, 1)\n",
    "        #inputs_proj = self.input_proj(inputs_reshaped)\n",
    "        #inputs_proj = inputs_proj.permute(0, 2, 1)\n",
    "        \n",
    "        #inputs_encoded = inputs_proj + self.pos_encoder\n",
    "        \n",
    "        transformer_output = self.transformer_encoder(inputs)\n",
    "        \n",
    "        output = self.linear_class(transformer_output[:, -1, :])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8477448-a033-4fa4-94d3-fdcf76bcb5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerModel                              [1, 64]                   --\n",
       "├─TransformerEncoder: 1-1                     [1, 40, 1662]             --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [1, 40, 1662]             17,873,534\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [1, 40, 1662]             17,873,534\n",
       "├─Linear: 1-2                                 [1, 64]                   106,432\n",
       "===============================================================================================\n",
       "Total params: 35,853,500\n",
       "Trainable params: 35,853,500\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.11\n",
       "===============================================================================================\n",
       "Input size (MB): 0.27\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.43\n",
       "Estimated Total Size (MB): 0.69\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = TransformerModel().to(device)\n",
    "summary(test_model, input_size=(1, 40, 1662))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb22d0c-ea3a-4253-97ea-388e9478ec5f",
   "metadata": {},
   "source": [
    "# Class for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205d42e-4ccf-4d26-a580-7a274d45984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reshaped = torch.reshape(X_train, (2560, -1))\n",
    "x_test_reshaped = torch.reshape(X_test, (640, -1))\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=10, random_state=42, max_depth= 64**3)\n",
    "random_forest_model.fit(x_train_reshaped, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f591f-b854-41fe-9249-51ce51fc3f4e",
   "metadata": {},
   "source": [
    "# Training the models and analysing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579c923-0062-492c-b878-bf9fd653ccb3",
   "metadata": {},
   "source": [
    "# 1. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4bbcf-8b9c-4b0b-9acd-97e414fde415",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 100\n",
    "batch_size= 128\n",
    "learning_rate= 0.0001\n",
    "\n",
    "lstm_model= LstmModel().to(device)                                   # initialize model\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= batch_size, num_workers=0, shuffle=True) \n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size= batch_size, num_workers=0, shuffle=False) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr= learning_rate)\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies, all_preds, all_labels = train(num_epochs, lstm_model, train_dataloader, test_dataloader, optimizer, criterion, calculate_accuracy, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac96588-ea6f-4692-9284-e2bd3f943af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c16c4e-464c-4125-b72a-b2bffcb3585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confmatrix(all_labels, all_preds, class_names, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b776c-f2f2-4927-96f6-47ce7eb6b783",
   "metadata": {},
   "source": [
    "# 2. Transformer with positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13e82a-c03d-4cfd-ac7c-3162fb607df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 120\n",
    "batch_size= 64\n",
    "learning_rate= 0.0001\n",
    "\n",
    "transformer_modelp= TransformerModelP().to(device)                  # initialize \n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= batch_size, num_workers=0, shuffle=True) \n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size= batch_size, num_workers=0, shuffle=False) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer_modelp.parameters(), lr= learning_rate)\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies, all_preds, all_labels = train(num_epochs, transformer_modelp, train_dataloader, test_dataloader, optimizer, criterion, calculate_accuracy, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3ed84-ec6e-43eb-aaf7-c8abda343526",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3b652-1d55-4cca-8bbc-ec3c42307cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confmatrix(all_labels, all_preds, class_names, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775af6d3-e99b-4f64-a0b4-7f6b6ae84954",
   "metadata": {},
   "source": [
    "# 3. Transformer with Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60331463-5aad-4f6d-9d94-6fbaa5dec578",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 120\n",
    "batch_size= 64\n",
    "learning_rate= 0.0001\n",
    "\n",
    "transformer_modelc= TransformerModelC().to(device)                  # initialize model\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= batch_size, num_workers=0, shuffle=True) \n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size= batch_size, num_workers=0, shuffle=False) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer_modelc.parameters(), lr= learning_rate)\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies, all_preds, all_labels = train(num_epochs, transformer_modelc, train_dataloader, test_dataloader, optimizer, criterion, calculate_accuracy, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c8352-6548-4a57-8201-a749f98882af",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec60392-f60e-4c25-b2c2-b5f8e65025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confmatrix(all_labels, all_preds, class_names, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06840b5f-cb26-447d-85cf-70e68d8f9595",
   "metadata": {},
   "source": [
    "# 4. Transformer with no positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ee084ec-4744-4b64-881b-29047e6107fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab69b5c1784a4e9d8c083cad5b41f088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(transformer_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m learning_rate)\n\u001b[1;32m---> 12\u001b[0m train_losses, test_losses, train_accuracies, test_accuracies, all_preds, all_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_accuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs, model, train_dataloader, test_dataloader, optimizer, criterion, calculate_accuracy, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 17\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     train_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m calculate_accuracy(outputs, labels)\n\u001b[0;32m     20\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs= 100\n",
    "batch_size= 128\n",
    "learning_rate= 0.0001\n",
    "\n",
    "transformer_model= TransformerModel().to(device)                  # initialize model\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= batch_size, num_workers=0, shuffle=True) \n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size= batch_size, num_workers=0, shuffle=False) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr= learning_rate)\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies, all_preds, all_labels = train(num_epochs, transformer_model, train_dataloader, test_dataloader, optimizer, criterion, calculate_accuracy, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "701bc037-278e-4d1e-b9f6-f0529eea3cb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m draw_loss_accuracy(\u001b[43mtrain_losses\u001b[49m, test_losses, train_accuracies, test_accuracies, batch_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "draw_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca443a-ee02-46c2-bb46-14144ec001d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confmatrix(all_labels, all_preds, class_names, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a11f3-8767-4320-8c1e-24dcc2a52574",
   "metadata": {},
   "source": [
    "# 6. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f19731-a519-4bb0-9eea-c9956da97caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = random_forest_model.score(x_test_reshaped, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac311e6-ad15-44d9-8481-6b2d38c79a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = random_forest_model.predict(x_test_reshaped)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5137b3b-33fc-41b4-8b26-a2a939d74d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
