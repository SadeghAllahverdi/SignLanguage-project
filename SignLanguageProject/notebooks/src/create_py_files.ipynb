{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa887d6-34db-49b7-af91-485398946b5d",
   "metadata": {},
   "source": [
    "This *.ipynb* file is used to \n",
    "1. create\n",
    "2. modify\n",
    "3. manage\n",
    "\n",
    "the *.py* files within the /src directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb828fd-0275-404b-abeb-ba99cd2203ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_directory: C:\\Users\\sadeg\\OneDrive\\Desktop\\Thesis\\python_codes\\SignLanguageProject\n",
      "src_directory: C:\\Users\\sadeg\\OneDrive\\Desktop\\Thesis\\python_codes\\SignLanguageProject\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# defining paths\n",
    "root_dir= os.path.dirname(os.getcwd())\n",
    "src_dir= os.path.join(root_dir, 'src')\n",
    "data_dir= os.path.join(root_dir, 'data')\n",
    "\n",
    "print(f'project_directory: {root_dir}')\n",
    "print(f'src_directory: {src_dir}')\n",
    "\n",
    "src_dir= src_dir.replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536632e-c5cd-4c84-9450-d7803d4fb334",
   "metadata": {},
   "source": [
    "# making \\_\\_init\\_\\_.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a509c252-cb5d-4350-9bee-d8b82f0311a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/__init__.py\n",
    "# Explanation: This file will mark the source directory as a python package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb9160-81a2-453a-a835-5752cf9c274a",
   "metadata": {},
   "source": [
    "# making prepare\\_datasets.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b657d7-7a72-492d-bfbb-230982e507a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/prepare_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/prepare_datasets.py\n",
    "# Explanation: This file will contain functions that are used to extract landmarks and make data sets\n",
    "# importing libraries for making datasets\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random \n",
    "\n",
    "import numpy as np                                                                  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt                                                       \n",
    "\n",
    "import cv2                                                                         \n",
    "import mediapipe as mp  \n",
    "\n",
    "from glob import glob                                                               \n",
    "from pathlib import Path                                                            \n",
    "from natsort import natsorted                                                       \n",
    "from tqdm.auto import tqdm                                                         \n",
    "from typing import Callable, List\n",
    "\n",
    "# function to get landmarks from LSA64 dataset.\n",
    "def get_landmarks_LSA64(root: str,\n",
    "                        class_names: List[str],                    \n",
    "                        frame_numbers: int):\n",
    "    \"\"\"\n",
    "    Goes through the LSA64_directory and creates a list of landmarks for each video within the directory by applying mediapipe model frame by frame.\n",
    "    Args:\n",
    "        root: Path to video dataset directory.\n",
    "        class_names: List of all words in the dataset.\n",
    "        frame_numbers: number of frames we want to take from the entire video: this is in principle fps.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of (detections, labels, len(all_video_paths),len(none_cv2_video_paths)).\n",
    "        Where detections is a list of all videos as mediapipe landmarks.\n",
    "        labels is a list of labels corresponding to each video detection.\n",
    "        len(all_video_paths),len(none_cv2_video_paths) are returned to see if cv2 was unable to open some\n",
    "        video files.\n",
    "        \n",
    "    \"\"\"\n",
    "    all_video_paths= natsorted([str(p) for p in Path(root).glob(\"**/*.mp4\")])      # makes list of video paths in the dataset\n",
    "    vid_idx_to_class_name= {i+1:label for i, label in enumerate(class_names)}      # this is used to change the video numbers to the class name since videos are encoded in a specific way\n",
    "    none_cv2_video_paths= []                                                       # stores video paths that cv2 couldnt open\n",
    "    detections= []                                                                 # stores detections\n",
    "    labels= []                                                                     # stores video labels\n",
    "    frame_numbers= frame_numbers\n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for video_path in tqdm(all_video_paths, desc=\"Processing videos\"):\n",
    "            cap = cv2.VideoCapture(video_path)              # Read each video using cv2\n",
    "            if not cap.isOpened():                          # if cv2 can't read the video\n",
    "                none_cv2_video_paths.append(video_path)     # save the video path\n",
    "            else:                                           # if cap can read the video\n",
    "                total_frames_number = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))                                # getting the total frames in video\n",
    "                frame_idxs_to_process = np.linspace(0, total_frames_number-1, frame_numbers, dtype=int)     # picking desiered frame indexes\n",
    "                video_detections= []\n",
    "                for idx in frame_idxs_to_process:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                    ret, frame= cap.read()\n",
    "                    #\n",
    "                    result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                    pose= np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4) \n",
    "                    face= np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3) \n",
    "                    lh= np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "                    rh= np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "                    detection= np.concatenate((pose,face,lh, rh))\n",
    "                    label= vid_idx_to_class_name[int(os.path.basename(video_path).split('_')[0])]\n",
    "                    video_detections.append(detection)\n",
    "\n",
    "                \n",
    "                detections.append(video_detections)    \n",
    "                label= vid_idx_to_class_name[int(os.path.basename(video_path).split('_')[0])] # gets the label for the videos\n",
    "                labels.append(label)\n",
    "   \n",
    "            cap.release()\n",
    "        \n",
    "    return detections, labels, len(all_video_paths),len(none_cv2_video_paths)\n",
    "\n",
    "# functions to get landmarks from WLASL100 dataset.\n",
    "\n",
    "def process_frame(cap, index):\n",
    "    \"\"\"\n",
    "    Positions the cv2 on a specific frame number, reads the frame and returns the results.\n",
    "    Example usage:\n",
    "        ret, frame= process_frame(cap, idx)\n",
    "    \"\"\"\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
    "    return cap.read() \n",
    "\n",
    "\n",
    "def get_landmarks_WLASL100(root: str,\n",
    "                           class_names: List[str],\n",
    "                           frame_numbers: int = 25):\n",
    "    \"\"\"\n",
    "    Goes through the WLASL100 directory and creates a list of landmarks for each video within the directory by applying mediapipe model frame by frame.\n",
    "    since some of the videos have faulty frames. it checks for before and after frames first. incase those are faulty as well it puts an empty list\n",
    "    for that frame of the video.\n",
    "    Args:\n",
    "        root: Path to video dataset directory.\n",
    "        class_names: List of all words in the dataset.\n",
    "        frame_numbers: number of frames we want to take from the entire video: this is in principle fps.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of (detections, labels, len(all_video_paths),len(none_cv2_video_paths)).\n",
    "        Where detections is a list of all videos as mediapipe landmarks.\n",
    "        labels is a list of labels corresponding to each video detection.\n",
    "        len(all_video_paths),len(none_cv2_video_paths) are returned to see if cv2 was unable to open some\n",
    "        video files.\n",
    "        Example usage:\n",
    "            results= get_landmarks_LSA64(root= 'C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/lsa64_raw/all',\n",
    "                                         class_names= class_names,\n",
    "                                         frame_numbers= 60): \n",
    "    \"\"\"\n",
    "    all_video_paths= natsorted([str(p) for p in Path(root).glob(\"**/*.mp4\")])\n",
    "    vid_idx_to_class_name= {i+1:label for i, label in enumerate(class_names)}\n",
    "    none_cv2_video_paths= []                                # keeping track of corrupt video files\n",
    "    detections= []                                          # saves mediapipe detections\n",
    "    labels= []\n",
    "    \n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for video_path in tqdm(all_video_paths, desc=\"Processing videos\"):\n",
    "            cap = cv2.VideoCapture(video_path)              # Read each video using cv2\n",
    "            if not cap.isOpened():                          # if cv2 can't read the video\n",
    "                none_cv2_video_paths.append(video_path)     # save the video path\n",
    "                continue\n",
    "                \n",
    "            total_frames_number = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))                                             # getting the total frames in video\n",
    "            frame_idxs_to_process = np.round(np.linspace(0, total_frames_number - 1, frame_numbers)).astype(int)     # picking desiered frame indexes\n",
    "            video_detections= []\n",
    "            for idx in frame_idxs_to_process:\n",
    "                ret, frame= process_frame(cap, idx)\n",
    "\n",
    "                if ret is False:\n",
    "                    # if the return value is False: meaning the frame was \"bad\".\n",
    "                    print(f\"Failed to grab frame {idx}, of video {video_path} of length {total_frames_number} frames. trying adjacent frames...\")\n",
    "\n",
    "                    # we try to read the previous frame.\n",
    "                    ret, frame = grab_frame(cap, idx - 1)\n",
    "                    if not ret:\n",
    "                        # if the return value is False: meaning previous frame was also \"bad\".\n",
    "                        # we try to read the next frame with cv2.\n",
    "                        ret, frame = grab_frame(cap, idx + 1)\n",
    "                        \n",
    "                if not ret:\n",
    "                    # if the return value is False.\n",
    "                    print(f\"Unable to retrieve any frames around index {idx}, of video {video_path} of length {total_frames_number} frames.\")\n",
    "                    # we add empty detection that will be filled using interpolation\n",
    "                    detection= []\n",
    "                    video_detections.append(detection)\n",
    "                    continue\n",
    "                            \n",
    "                result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                pose= np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4) \n",
    "                face= np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3) \n",
    "                lh= np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "                rh= np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "                detection= np.concatenate((pose,face,lh, rh))\n",
    "                video_detections.append(detection)\n",
    "\n",
    "                \n",
    "            detections.append(video_detections)    \n",
    "            label= vid_idx_to_class_name[int(os.path.basename(os.path.dirname(video_path)))]\n",
    "            labels.append(label)\n",
    "   \n",
    "        cap.release()\n",
    "        \n",
    "    return detections, labels, len(all_video_paths),len(none_cv2_video_paths)\n",
    "\n",
    "\n",
    "# functions to interpolate faulty video frames in the dataset.\n",
    "def interpolate_frames(most_recent_detection, next_coming_detection, alpha):\n",
    "    \"\"\"\n",
    "    Based on the value of most recent detection and next coming detection which are the frames before and after our faulty frame returns \n",
    "    a landmark array for the faulty frame.\n",
    "    Args:\n",
    "        most_recent_detection: landmarks detected in previous frame.\n",
    "        next_coming_detection: landmarks detected in the next frame.\n",
    "        alpha: interpolation factor.\n",
    "        \n",
    "    Returns:\n",
    "        either: (1 - alpha) * most_recent_detection + alpha * next_coming_detection\n",
    "        or: next_coming_detection\n",
    "        or: most_recent_detection\n",
    "\n",
    "    Example usage:\n",
    "        video_detection[i]= interpolate_frames(most_recent_detection, next_coming_detection, 0.5)\n",
    "    \"\"\"\n",
    "    if most_recent_detection is None and next_coming_detection is not None:             # first to nth frames are all corrupt\n",
    "        return next_coming_detection\n",
    "    elif most_recent_detection is not None and next_coming_detection is None:           # nth to last frames are all corrupt\n",
    "        return most_recent_detection\n",
    "    else:\n",
    "        return (1 - alpha) * most_recent_detection + alpha * next_coming_detection \n",
    "\n",
    "\n",
    "def fill_empty_detections(result):\n",
    "    \"\"\"\n",
    "    In principle fills up the empty landmark detections for frames that where faulty in the dataset and returns the dataset .\n",
    "    Args:\n",
    "        detections\n",
    "        \n",
    "    Returns:\n",
    "        detections (with no empty landmark frame)\n",
    "\n",
    "    Example usage: detections= fill_empty_detections(detections)\n",
    "    \"\"\"\n",
    "    detections= result\n",
    "    for video_detection in detections:\n",
    "        most_recent_detection= None\n",
    "        for i in range(len(video_detection)):\n",
    "            if len(video_detection[i]) != 0:\n",
    "                most_recent_detection= video_detection[i]\n",
    "            else:\n",
    "                next_coming_detection= None\n",
    "                for j in range(i+1, len(video_detection)):\n",
    "                    if len(video_detection[j]) != 0:\n",
    "                        next_coming_detection= video_detection[j]\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                     \n",
    "                video_detection[i]= interpolate_frames(most_recent_detection, next_coming_detection, 0.5)\n",
    "                most_recent_detection= video_detection[i]\n",
    "\n",
    "    return detections\n",
    "\n",
    "wlasl100class_names= [\"accident\", \"africa\", \"all\", \"apple\", \"basketball\", \"bed\", \"before\", \"bird\", \"birthday\",\n",
    "                      \"black\", \"blue\", \"bowling\", \"brown\", \"but\", \"can\", \"candy\", \"chair\", \"change\", \"cheat\", \"city\",\n",
    "                      \"clothes\", \"color\", \"computer\", \"cook\", \"cool\", \"corn\", \"cousin\", \"cow\", \"dance\", \"dark\",\n",
    "                      \"deaf\", \"decide\", \"doctor\", \"dog\", \"drink\", \"eat\", \"enjoy\", \"family\", \"fine\", \"finish\",\n",
    "                      \"fish\", \"forget\", \"full\", \"give\", \"go\", \"graduate\", \"hat\", \"hearing\", \"help\", \"hot\",\n",
    "                      \"how\", \"jacket\", \"kiss\", \"language\", \"last\", \"letter\", \"like\", \"man\", \"many\", \"meet\",\n",
    "                      \"mother\", \"need\", \"no\", \"now\", \"orange\", \"paint\", \"paper\", \"pink\", \"pizza\", \"play\",\n",
    "                      \"pull\", \"purple\", \"right\", \"same\", \"school\", \"secretary\", \"shirt\", \"short\", \"son\", \"study\",\n",
    "                      \"table\", \"tall\", \"tell\", \"thanksgiving\", \"thin\", \"thursday\", \"time\", \"walk\", \"want\", \"what\",\n",
    "                      \"white\", \"who\", \"woman\", \"work\", \"wrong\", \"year\", \"yes\", \"book\", \"later\", \"medicine\"]\n",
    "\n",
    "\n",
    "lsa64class_names= ['Opaque', 'Red', 'Green', 'Yellow', 'Bright', 'Light-blue', 'Colors', 'Pink',\n",
    "                   'Women', 'Enemy', 'Son', 'Man', 'Away', 'Drawer', 'Born', 'Learn',\n",
    "                   'Call', 'Skimmer', 'Bitter', 'Sweet milk', 'Milk', 'Water', 'Food', 'Argentina',\n",
    "                   'Uruguay', 'Country', 'Last name', 'Where', 'Mock', 'Birthday', 'Breakfast', 'Photo',\n",
    "                   'Hungry', 'Map', 'Coin', 'Music', 'Ship', 'None', 'Name', 'Patience',\n",
    "                   'Perfume', 'Deaf', 'Trap', 'Rice', 'Barbecue', 'Candy', 'Chewing-gum', 'Spaghetti',\n",
    "                   'Yogurt', 'Accept', 'Thanks', 'Shut down', 'Appear', 'To land', 'Catch', 'Help',\n",
    "                   'Dance', 'Bathe', 'Buy', 'Copy', 'Run', 'Realize', 'Give', 'Find']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04e0b6-e0d7-4b7a-92eb-acfc5d03939b",
   "metadata": {},
   "source": [
    "# making preprocessing.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "259a9e47-5778-4fdd-b2d7-266a491350b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/preprocessing.py\n",
    "# importing libraries for preprocessing data\n",
    "import os                                                                           \n",
    "import random \n",
    "import json\n",
    "import numpy as np                                                                  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt                                                       \n",
    "\n",
    "import cv2                                                                         \n",
    "import mediapipe as mp  \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob                                                               \n",
    "from pathlib import Path                                                            \n",
    "from natsort import natsorted                                                       \n",
    "from tqdm.auto import tqdm                                                         \n",
    "from collections import defaultdict\n",
    "from typing import Callable, List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# functions to implement interpolation \n",
    "def interpolate_video_detections(vd1: NDArray[np.float64], \n",
    "                                 vd2: NDArray[np.float64], \n",
    "                                 shapes: List[Tuple[int, int]],\n",
    "                                 alpha: float):\n",
    "    \"\"\"\n",
    "    This function will return an Array which is in principle the interpolation of 2 video detection arrays.\n",
    "    Args:\n",
    "        vd1: landmarks detected in first video.\n",
    "        vd2: landmarks detected in second video.\n",
    "        shapes: represents the start and end index for each landmark class: head , pose, left hand and right hand.\n",
    "        alpha: interpolation factor\n",
    "        \n",
    "    Returns:\n",
    "        either: (1 - alpha) * most_recent_detection + alpha * next_coming_detection\n",
    "        or: next_coming_detection\n",
    "        or: most_recent_detection\n",
    "\n",
    "    Example usage: \n",
    "        \n",
    "    \"\"\"\n",
    "    num_frames = vd1.shape[0]\n",
    "    ivd= np.zeros_like(vd1)\n",
    "    for i in range(num_frames):\n",
    "        fd1= vd1[i]    # frame in video 1\n",
    "        fd2= vd2[i]    # frame in video 2\n",
    "        ifd= np.zeros_like(fd1)  # stores interpolated frame\n",
    "        for (start, end) in shapes:         # goes through each landmark class and interpolates\n",
    "            fd1part= fd1[start:end]\n",
    "            fd2part= fd2[start:end]\n",
    "            if np.all(fd1part == 0) and np.all(fd2part == 0): # all pose or face or lh or rh set of landmarks are 0 or not detected\n",
    "                ivd[i][start:end] = np.zeros(end- start)      # put zero\n",
    "            elif np.all(fd1part == 0):                        # all pose or face or lh or rh only in frame of video 1 is are not detected\n",
    "                ivd[i][start:end] = fd2part                   # use video 2 pose or face or lh or rh\n",
    "            elif np.all(fd2part == 0):                        # all pose or face or lh or rh only in frame of video 2 is are not detected\n",
    "                ivd[i][start:end] = fd1part                   # use video 1 pose or face or lh or rh\n",
    "            else:\n",
    "                #ivd[i][start:end]= (1 - alpha) * fd1part + alpha * fd2part\n",
    "                # this formula also works very nice\n",
    "                A = fd1part+ ((fd1part + fd2part) / 2)**2 - (fd1part)**2 + ((fd1part + fd2part) / 2)**2 - (fd2part)**2  # Interpolate normally\n",
    "                B = fd1part+ ((fd1part + fd2part) / 2)**2 - (fd1part)**2 + ((fd1part + fd2part) / 2)**2 - (fd2part)**2 \n",
    "                ivd[i][start:end]= (1 - alpha) * A + alpha * B\n",
    "    return ivd\n",
    "\n",
    "\n",
    "def interpolate_dataset(detections: NDArray[np.float64],\n",
    "                        labels: List[str],\n",
    "                        alpha: float= 0.5,\n",
    "                        min_interpolations: int= 5):\n",
    "    \"\"\"\n",
    "    This function does interpolation on videos with same label across the dataset. \n",
    "    Args:\n",
    "        detections: landmark dataset array.\n",
    "        labels: list of labels.\n",
    "        alpha: interpolation factor.\n",
    "        min_interpolations: minimum number of interpolated videos produced for each label\n",
    "        \n",
    "    Returns:\n",
    "        a tuple of (np.array(x), y) where np.array(x) is the landmark dataset array and\n",
    "        y is the corresponding label for each video detection in the landmark dataset array\n",
    "\n",
    "    Example usage:\n",
    "        ivd = interpolate_video_detections(vid1, vid2, [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)],0.5, 5) \n",
    "    \"\"\"\n",
    "    data= defaultdict(list)                         # stores current data\n",
    "    interpolated_data= defaultdict(list)            # stores interpolated data\n",
    "    augumented_data = defaultdict(list)             # union of current and interpolated\n",
    "    detection_shape= [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)]  # represents how face, pose, lh, rh are stored\n",
    "    \n",
    "    x = []                                          # used to unpack augumented_data\n",
    "    y = []\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        data[label].append(detections[idx])\n",
    "\n",
    "    for label, detections in data.items():\n",
    "        pairs= []\n",
    "        for i in range(len(detections)):\n",
    "            for j in range(i+1, len(detections)):\n",
    "                pairs.append((i, j))\n",
    "        selected_pairs = random.sample(pairs, min(min_interpolations, len(pairs)))\n",
    "        for (i, j) in selected_pairs:\n",
    "            ivd = interpolate_video_detections(detections[i], detections[j], detection_shape,alpha)\n",
    "            interpolated_data[label].append(ivd)\n",
    "\n",
    "    for d in (data, interpolated_data):\n",
    "        for label, detections in d.items():\n",
    "            augumented_data[label].extend(detections)\n",
    "\n",
    "    for label, detections in augumented_data.items():\n",
    "        for detection in detections:\n",
    "            x.append(detection)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(x), y\n",
    "\n",
    "\n",
    "# functions for drawing video landmarks\n",
    "def draw_landmarks(frame: np.ndarray,\n",
    "                   detection: NDArray[np.float64],\n",
    "                   structure: List[int]):\n",
    "    \"\"\"\n",
    "    This function traverses in the video array and draws the detections frame by frame.  \n",
    "    Args:\n",
    "        frame: represents frame that is shown\n",
    "        detection: array that represents video frame by frame.\n",
    "        structure list of integers where each entary says how many dimention does this landmark has: \n",
    "        \n",
    "    Returns:\n",
    "        manipulated frame \n",
    "\n",
    "    Example usage:\n",
    "        frame = draw_landmarks(frame, vid, [4]*33 + [3]*(468 + 21 + 21))\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    while idx < len(detection):\n",
    "        for coordinates in structure:\n",
    "            if idx + coordinates > len(detection): # makes sure the index stays in the video\n",
    "                break  \n",
    "\n",
    "            x, y = detection[idx], detection[idx + 1] # get x, y values\n",
    "            # multiply x and y to hight and width of the frame\n",
    "            px = int(x * frame.shape[1]) \n",
    "            py = int(y * frame.shape[0])\n",
    "            cv2.circle(frame, (px, py), 3, (0, 255, 0), -1) # draws the landmark\n",
    "            \n",
    "            idx += coordinates # base on value of coordinate (either 3 or 4)jumps to next landmark in principle\n",
    "            if idx >= len(detection): \n",
    "                break\n",
    "    return frame\n",
    "\n",
    "\n",
    "def show_detections(vd: NDArray[np.float64]):\n",
    "    \"\"\"\n",
    "    This function draws x and y landmarks of a video .  \n",
    "    Args:\n",
    "        vd: an array that represents video detections \n",
    "    \"\"\"\n",
    "    structure= [4]*33 + [3]*(468 + 21 + 21)  \n",
    "    height= 720\n",
    "    width= 1280\n",
    "    cv2.namedWindow(\"Landmark Preview\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"Landmark Preview\", width= width, height= height)\n",
    "    try:\n",
    "        for idx, detection in enumerate(vd):\n",
    "            frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            frame = draw_landmarks(frame, detection, structure)\n",
    "            cv2.imshow(\"Landmark Preview\", frame)\n",
    "            if cv2.waitKey(100) & 0xFF == 27:  #ESC key\n",
    "                break\n",
    "            #print(f\"Displaying frame {idx + 1}/{len(vd)}\")\n",
    "    finally:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def split_dataset(detections: NDArray[np.float64],\n",
    "                  labels: List[str],\n",
    "                  class_names: List[str],\n",
    "                  test_size: float,\n",
    "                  random_state: int):\n",
    "    \"\"\"\n",
    "    splits the dataset and maps each video label to a corresponding number that is suitable for training process.  \n",
    "    Args:\n",
    "        detections: mediapipe detections for entire dataset.\n",
    "        labels: list of all video labels for the entire dataset.\n",
    "        class_names: list of all class names in the dataset\n",
    "        test_size: determines how data should be splitted\n",
    "        random_state:\n",
    "        \n",
    "    Returns:\n",
    "        a tuple of (X_train, X_test, y_train, y_test) \n",
    "\n",
    "    Example usage:\n",
    "        xtrain, xtest, ytrain, ytest= split_dataset(detections, labels, class_names, 0.2, 42)\n",
    "    \"\"\"\n",
    "    label_map= {label: num for num, label in enumerate(class_names)}\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(detections, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "    X_train, X_test= torch.tensor(X_train, dtype=torch.float32) , torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    y_train= [label_map[label] for label in y_train]\n",
    "    y_test= [label_map[label] for label in y_test]\n",
    "    y_train= torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test= torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e7a2b-e15f-4b3b-940f-55afbe116636",
   "metadata": {},
   "source": [
    "# making models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e1b868-aa25-48a3-9a3a-aa6263fed902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/models.py\n",
    "# importing libraries necessary for creating the models\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import List\n",
    "\n",
    "# Transformer classes\n",
    "\n",
    "class ParamTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 class_names: List[str],\n",
    "                 seq_len: int= 40, \n",
    "                 d_model: int= 1662,\n",
    "                 nhead: int= 6,\n",
    "                 d_ff: int=2048,\n",
    "                 num_layers: int= 2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer model with learable parameter for positional encoding'\n",
    "        self.class_names= class_names\n",
    "        self.positional_encoding= nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model= d_model, nhead= nhead, dim_feedforward= d_ff, batch_first= True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer= self.encoder_layer, num_layers= num_layers)\n",
    "        self.classifier = nn.Linear(in_features= d_model, out_features= len(self.class_names))\n",
    "\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape ``[batch_size, seq_len, input_shape]``\n",
    "        Returns:\n",
    "            output: Tensor of shape ``[batch_size, len(class_names)]``\n",
    "        \"\"\"\n",
    "        output = src+ self.positional_encoding\n",
    "        output = self.transformer_encoder(output)\n",
    "        output = self.classifier(output[:,-1,:])\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class LinearParamTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 class_names: List[str],\n",
    "                 seq_len: int= 40, \n",
    "                 d_model: int= 128,\n",
    "                 nhead: int= 4,\n",
    "                 d_ff: int=2048,\n",
    "                 num_layers: int= 2,\n",
    "                 input_shape: int= 1662):\n",
    " \n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer model with linear layer and learnable parameter for positional encoding'\n",
    "        \n",
    "        self.class_names= class_names\n",
    "        self.positional_encoding= nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        self.linear= nn.Linear(in_features= input_shape, out_features= d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model= d_model, nhead= nhead, dim_feedforward= d_ff, batch_first= True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer= self.encoder_layer, num_layers= num_layers)\n",
    "        self.classifier = nn.Linear(in_features= d_model, out_features= len(self.class_names))\n",
    "\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape ``[batch_size, seq_len, input_shape]``\n",
    "        Returns:\n",
    "            output: Tensor of shape ``[batch_size, len(class_names)]``\n",
    "        \"\"\"\n",
    "        output = self.linear(src)\n",
    "        output= output+ self.positional_encoding\n",
    "        output = self.transformer_encoder(output)\n",
    "        output = self.classifier(output[:,-1,:])\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ConvoTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 class_names: List[str],\n",
    "                 seq_len: int= 40, \n",
    "                 d_model: int= 128,\n",
    "                 nhead: int= 4,\n",
    "                 d_ff: int=2048,\n",
    "                 num_layers: int= 2,\n",
    "                 input_shape: int= 1662,\n",
    "                 kernel_size: int= 1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer model with convolutional layer for positional encoding'\n",
    "        self.class_names= class_names\n",
    "        self.positional_encoding= nn.Conv1d(in_channels= input_shape, out_channels= d_model, kernel_size= kernel_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model= d_model, nhead= nhead, dim_feedforward= d_ff, batch_first= True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer= self.encoder_layer, num_layers= num_layers)\n",
    "        self.classifier = nn.Linear(in_features= d_model, out_features= len(self.class_names))\n",
    "\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape ``[batch_size, seq_len, input_shape]``\n",
    "        Returns:\n",
    "            output: Tensor of shape ``[batch_size, len(class_names)]``\n",
    "        \"\"\"\n",
    "        src= src.permute(0, 2, 1)\n",
    "        output = self.positional_encoding(src)\n",
    "        output = output.permute(0, 2, 1)\n",
    "        output = self.transformer_encoder(output)\n",
    "        output = self.classifier(output[:,-1,:])\n",
    "        return output\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 class_names: List[str],\n",
    "                 seq_len: int= 40, \n",
    "                 d_model: int= 1662,\n",
    "                 nhead: int= 6,\n",
    "                 d_ff: int=2048,\n",
    "                 num_layers: int= 2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer model without any positional encoding'\n",
    "        self.class_names= class_names\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model= d_model, nhead= nhead, dim_feedforward= d_ff, batch_first= True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer= self.encoder_layer, num_layers= num_layers)\n",
    "        self.classifier = nn.Linear(in_features= d_model, out_features= len(self.class_names))\n",
    "\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape ``[batch_size, seq_len, input_shape]``\n",
    "        Returns:\n",
    "            output: Tensor of shape ``[batch_size, len(class_names)]``\n",
    "        \"\"\"\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.classifier(output[:,-1,:])\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "#Lstm classes\n",
    "class LstmModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 class_names: List[str],\n",
    "                 input_size: int= 1662,\n",
    "                 hidden_size: int= 277,\n",
    "                 num_layers: int= 1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.class_names= class_names\n",
    "        self.lstm_layers= nn.ModuleList()\n",
    "\n",
    "        self.lstm_layers.append(nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True))\n",
    "        for _ in range(1, num_layers):\n",
    "            self.lstm_layers.append(nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True))\n",
    "        \n",
    "        self.fc = nn.Linear(in_features= hidden_size, out_features= len(self.class_names))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape ``[batch_size, seq_len, input_shape]``\n",
    "            \n",
    "        Returns:\n",
    "            output: Tensor of shape ``[batch_size, len(class_names)]``\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        for lstm in self.lstm_layers:\n",
    "            output, _ = lstm(output)\n",
    "            output = self.relu(output)\n",
    "\n",
    "        output= self.fc(output[:,-1,:])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d19870-fe1e-46f4-b5da-fce81f868982",
   "metadata": {},
   "source": [
    "# making training.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8675fe6a-7a6e-4f77-9409-879eeacff80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/training.py\n",
    "#importing libraries for training models\n",
    "import torch\n",
    "import torch.optim as optim                                                          \n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader     \n",
    "from typing import Callable, List\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# function to calculate accuracy\n",
    "def accuracy_fn(y_logits: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    returns accuracy based on true and predicted label values\n",
    "    Args:\n",
    "        y_logits: torch tensor that represents model outputs\n",
    "        y: torch tensor that represents true output values\n",
    "    Returns:\n",
    "        accuracy\n",
    "\n",
    "    Example usage: \n",
    "        accuracy= accuracy_fn(y_logits, y)\n",
    "    \"\"\"\n",
    "    y_preds= torch.argmax(y_logits, 1)\n",
    "    corrects= (y_preds==y)\n",
    "    accuracy= corrects.sum().item()/ corrects.size(0)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# function to train the model\n",
    "def train(num_epochs: int,\n",
    "          model: torch.nn.Module,\n",
    "          train_dataloader: DataLoader,\n",
    "          test_dataloader: DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          accuracy_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "          device: torch.device):\n",
    "    \"\"\"\n",
    "    Trains a model on the given train and test data sets.\n",
    "    Args:\n",
    "        num_epochs: number of epoches the model is trained\n",
    "        model: model object\n",
    "        train_dataloader: DataLoader object that enables easy access to the samples of train dataset\n",
    "        test_dataloader: DataLoader object that enables easy access to the samples of test dataset.\n",
    "        optimizer: represents optimizing algorithms.\n",
    "        loss_fn: function that calculates loss\n",
    "        accuracy_fn: function that calculates accuracy\n",
    "        device: either Cuda or Cpu based on settings\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of (train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds).\n",
    "        Where :\n",
    "        train_losses is a list losses over based on train dataset over all epochs.\n",
    "        test_losses is a list losses over based on test dataset over all epochs.\n",
    "        train_accuracies is a list accuracies over based on train dataset over all epochs.\n",
    "        test_accuracies is a list accuracies over based on test dataset over all epochs.\n",
    "        y_trues, y_preds are used to draw confusion matrix (they are overwritten in each epoch so in principle the value of\n",
    "        of y_trues and y_preds is returned for the last epoch).\n",
    "\n",
    "    Example usage: \n",
    "        results= train(num_epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, accuracy_fn, device)\n",
    "    \"\"\"\n",
    "    \n",
    "    train_losses= []          # stores avg train losses of epoch\n",
    "    test_losses= []           # stores avg test losses of epoch \n",
    "    train_accuracies= []      # stores avg train acc of epoch\n",
    "    test_accuracies= []       # # stores avg test acc of epoch \n",
    " \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Epoch\"):\n",
    "        model.train()\n",
    "        train_loss= [] # stores avg loss of batch\n",
    "        train_acc= []  # stores avg acc of batch\n",
    "\n",
    "        for X, y in train_dataloader:\n",
    "            X= X.to(device) \n",
    "            y= y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_logits = model(X)\n",
    "            loss = loss_fn(y_logits, y)        # loss of the batch\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracy= accuracy_fn(y_logits, y) # accuracy of the batch\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(accuracy)\n",
    "        \n",
    "        train_losses.append(sum(train_loss) / len(train_loss))\n",
    "        train_accuracies.append(sum(train_acc) / len(train_acc))\n",
    "    \n",
    "        model.eval()\n",
    "\n",
    "        y_trues= []       # stores all the true labels for conf matrix\n",
    "        y_preds= []       # stores all the predictions for conf matrix\n",
    "        test_loss= []       # avg batch loss in test data per epoch\n",
    "        test_acc= []        # avg batch acc in test data per epoch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in test_dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                y_logits = model(X)\n",
    "                loss = loss_fn(y_logits, y)\n",
    "                accuracy= accuracy_fn(y_logits, y)\n",
    "                \n",
    "                test_loss.append(loss.item())\n",
    "                test_acc.append(accuracy)\n",
    "                y_pred= torch.argmax(y_logits, 1)                  # predicted labels\n",
    "                \n",
    "                y_trues.extend(y.view(-1).cpu().numpy())           # Store true labels\n",
    "                y_preds.extend(y_pred.view(-1).cpu().numpy())     # Store predictions\n",
    "                \n",
    "        \n",
    "        test_losses.append(sum(test_loss) / len(test_loss))\n",
    "        test_accuracies.append(sum(test_acc) / len(test_acc))\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds\n",
    "\n",
    "\n",
    "# function for drawing loss and accuracy of a training session\n",
    "def plot_loss_accuracy(train_losses: List[float],\n",
    "                       test_losses: List[float],\n",
    "                       train_accuracies: List[float],\n",
    "                       test_accuracies: List[float],\n",
    "                       batch_size: int):\n",
    "    \"\"\"\n",
    "    Draws loss and accuracy of a training session.\n",
    "    Args:\n",
    "        train_losses: list of train losses\n",
    "        test_losses: list of test losses\n",
    "        train_accuracies: list of train accuracies\n",
    "        test_accuracies: list of test accuracies\n",
    "        batch_size: batch size\n",
    "\n",
    "    Example usage:\n",
    "        plot_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, 64)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 9))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1) \n",
    "    plt.plot(train_losses, label='Train Loss')  \n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title(f'Loss over Epochs(batch size= {batch_size}), Last Loss:{test_losses[-1]}, ')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(test_accuracies, label='Test Accuracy')\n",
    "    plt.title(f'Accuracy over Epochs(batch size= {batch_size}), Last Accuracy: {test_accuracies[-1]}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# function for drawing confusion matrix\n",
    "def plot_confusion_matrix(y_trues: List[int],\n",
    "                          y_preds: List[int],\n",
    "                          class_names: List[str],\n",
    "                          num_epochs: int):\n",
    "    \"\"\"\n",
    "    Trains a model on the given train and test data sets.\n",
    "    Args:\n",
    "        y_trues: true values\n",
    "        y_preds: model predictions\n",
    "        class_names: list of all class names in the dataset\n",
    "        num_epochs: number of epochs\n",
    "    Example usage:\n",
    "        plot_confusion_matrix(y_trues, y_preds, class_names, num_epochs)\n",
    "    \"\"\"\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_trues, y_preds)\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    \n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted classes')\n",
    "    plt.ylabel('Actual classes')\n",
    "    \n",
    "    plt.title(f'Confusion Matrix after {num_epochs} epoches')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47914b-7b32-457f-be29-b83a50c1d156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
