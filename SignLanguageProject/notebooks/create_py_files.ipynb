{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa887d6-34db-49b7-af91-485398946b5d",
   "metadata": {},
   "source": [
    "This *.ipynb* file is used to \n",
    "1. create\n",
    "2. modify\n",
    "3. manage\n",
    "\n",
    "the *.py* files within the /src directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb828fd-0275-404b-abeb-ba99cd2203ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_directory: C:\\Users\\sadeg\\OneDrive\\Desktop\\Thesis\\python_codes\\SignLanguageProject\n",
      "src_directory: C:\\Users\\sadeg\\OneDrive\\Desktop\\Thesis\\python_codes\\SignLanguageProject\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# defining paths\n",
    "root_dir= os.path.dirname(os.getcwd())\n",
    "src_dir= os.path.join(root_dir, 'src')\n",
    "data_dir= os.path.join(root_dir, 'data')\n",
    "\n",
    "print(f'project_directory: {root_dir}')\n",
    "print(f'src_directory: {src_dir}')\n",
    "\n",
    "src_dir= src_dir.replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536632e-c5cd-4c84-9450-d7803d4fb334",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making \\_\\_init\\_\\_.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a509c252-cb5d-4350-9bee-d8b82f0311a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/__init__.py\n",
    "# Explanation: This file will mark the source directory as a python package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb9160-81a2-453a-a835-5752cf9c274a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making prepare\\_datasets.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9b657d7-7a72-492d-bfbb-230982e507a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/prepare_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/prepare_datasets.py\n",
    "# Explanation: This python file contains functions that are used to extract landmarks from LSA64, AUTSL40 and WLASL100 datasets.\n",
    "\n",
    "#------------------------------------------------------------------------------Import--------------------------------------------------------------------------\n",
    "\n",
    "# importing libraries for working with directories of the libreries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from natsort import natsorted \n",
    "# importing OpenCV and Mediapipe to read videos and extract landmarks\n",
    "import cv2                                                                         \n",
    "import mediapipe as mp  \n",
    "# importing numpy to work with arrays\n",
    "import numpy as np                                                                                                                           \n",
    "# importing tqdm for progression bar and typing for writing input types for each function                                                      \n",
    "from tqdm.auto import tqdm                                                         \n",
    "from typing import Callable, List\n",
    "\n",
    "#------------------------------------------------------------------constant variables--------------------------------------------------------------------------\n",
    "# A list for all class names in AUTSL 40\n",
    "autslclass_names = [\"sister\", \"hurry\", \"hungry\", \"enjoy_your_meal\", \"brother\", \"tree\", \"heavy\", \"cry\", \"family\", \"wise\", \"unwise\", \"kin\", \"shopping\", \"key\",\n",
    "                    \"mother\", \"friend\", \"ataturk\", \"shoe\", \"mirror\", \"same\", \"father\", \"garden\", \"look\", \"honey\", \"glass\", \"flag\", \"feast\", \"baby\", \"single\",\n",
    "                    \"wait\", \"I\", \"petrol\", \"together\", \"inform\", \"we\", \"work\", \"wednesday\", \"fork\", \"tea\", \"teapot\"]\n",
    "\n",
    "# A list for all class names in LSA64\n",
    "lsa64class_names= ['Opaque', 'Red', 'Green', 'Yellow', 'Bright', 'Light-blue', 'Colors', 'Pink', 'Women', 'Enemy', 'Son', 'Man', 'Away', 'Drawer', 'Born',\n",
    "                   'Learn', 'Call', 'Skimmer', 'Bitter', 'Sweet milk', 'Milk', 'Water', 'Food', 'Argentina', 'Uruguay', 'Country', 'Last name', 'Where',\n",
    "                   'Mock', 'Birthday', 'Breakfast', 'Photo', 'Hungry', 'Map', 'Coin', 'Music', 'Ship', 'None', 'Name', 'Patience','Perfume', 'Deaf', 'Trap',\n",
    "                   'Rice', 'Barbecue', 'Candy', 'Chewing-gum', 'Spaghetti', 'Yogurt', 'Accept', 'Thanks', 'Shut down', 'Appear', 'To land', 'Catch', 'Help',\n",
    "                   'Dance', 'Bathe', 'Buy', 'Copy', 'Run', 'Realize', 'Give', 'Find']\n",
    "\n",
    "# A list for all class names in WLALS100\n",
    "wlasl100class_names = [\"accident\", \"africa\", \"all\", \"apple\", \"basketball\", \"bed\", \"before\", \"bird\", \"birthday\", \"black\", \"blue\", \"book\", \"bowling\", \"brown\",\n",
    "                       \"but\", \"can\", \"candy\", \"chair\", \"change\", \"cheat\", \"city\", \"clothes\", \"color\", \"computer\", \"cook\", \"cool\", \"corn\", \"cousin\", \"cow\",\n",
    "                       \"dance\", \"dark\", \"deaf\", \"decide\", \"doctor\", \"dog\", \"drink\",\"eat\", \"enjoy\", \"family\", \"fine\", \"finish\", \"fish\", \"forget\", \"full\",\n",
    "                       \"give\", \"go\", \"graduate\", \"hat\", \"hearing\", \"help\", \"hot\", \"how\", \"jacket\", \"kiss\", \"language\", \"last\", \"later\", \"letter\", \"like\",\n",
    "                       \"man\", \"many\", \"medicine\", \"meet\", \"mother\", \"need\", \"no\", \"now\", \"orange\", \"paint\", \"paper\", \"pink\", \"pizza\", \"play\", \"pull\", \"purple\",\n",
    "                       \"right\",\"same\", \"school\", \"secretary\", \"shirt\", \"short\", \"son\", \"study\", \"table\", \"tall\", \"tell\", \"thanksgiving\", \"thin\", \"thursday\",\n",
    "                       \"time\", \"walk\", \"want\", \"what\", \"white\", \"who\", \"woman\", \"work\", \"wrong\", \"year\", \"yes\"]\n",
    "\n",
    "top_35_indexes= [2, 3, 4, 7, 9, 10, 11, 13, 15, 20, 22, 27, 29, 35, 36, 42, 48, 50, 53, 54, 58, 59, 62, 63, 66, 67, 69, 77, 81, 82, 90, 93, 94, 95, 99]\n",
    "wlasl35class_names= [wlasl100class_names[i] for i in top_35_indexes]\n",
    "#--------------------------------------------------------------------Getting landmarks--------------------------------------------------------------------------\n",
    "# function to get landmarks from LSA64 or AUTSL40 dataset.\n",
    "def get_landmarks(root: str,\n",
    "                  class_names: List[str],\n",
    "                  frame_numbers: int):\n",
    "    \"\"\"\n",
    "    This function retrieves all video paths from the dataset directory. Then the function analysis videos frame by frame and extract landmark. Finally the\n",
    "    function is able to assigne each video, an array of detected landmarks. depending on the datas, the function also uses the title of each video to assign\n",
    "    labels to them by using a dictionary.\n",
    "    Args:\n",
    "        root: Path to where dataset is located.\n",
    "        class_names: List of all words in the dataset.\n",
    "        frame_numbers: number of frames we want to take from the each video in the dataset.\n",
    "    Returns:\n",
    "        detections, labels, len(all_video_paths),len(none_cv2_video_paths) where:\n",
    "        detections: is a list of all mediapipe landmarks that were detected from all videos.\n",
    "        labels: is a list of labels corresponding to each video detection.\n",
    "        len(all_video_paths): is the number of videos in the dataset.\n",
    "        none_cv2_video_paths\" is a list of videos that OpenCV was not able to open.\n",
    "    Example use:\n",
    "        results= get_landmarks_LSA64(root= root, class_names= lsa64class_names, frame_numbers= 30)\n",
    "        detections, labels, num_all_videos, none_cv2_video_paths= results[0], results[1], results[2], results[3]\n",
    "    \"\"\"\n",
    "    labels= []                       # a list to store video labels\n",
    "    detections= []                   # a list to store all video detections\n",
    "    none_cv2_video_paths= []         # a list to store video paths that cv2 can't capture\n",
    "    \n",
    "    all_video_paths= Path(root).glob(\"**/*.mp4\")                           # a list to store all video paths in the dataset\n",
    "    all_video_paths= [str(path) for path in all_video_paths]               # changing path objects to strings since natosrt works with strings\n",
    "    all_video_paths= natsorted(all_video_paths)                            # sorted\n",
    "    vid_idx_to_label= {i:label for i, label in enumerate(class_names)}     # this mapping is used to change the video titles to labels\n",
    "    \n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for video_path in tqdm(all_video_paths, desc=\"Processing videos\"):\n",
    "            cap = cv2.VideoCapture(video_path)                             # capture each video using OpenCV\n",
    "            if not cap.isOpened():                                         # if OpenCV can't capture the video path\n",
    "                none_cv2_video_paths.append(video_path)                    # add the video path to none_cv2_video_paths\n",
    "            else:                                                                                                      \n",
    "                video_detections= []                                                                     # a list to store video detections\n",
    "                total_frames_number= cap.get(cv2.CAP_PROP_FRAME_COUNT)                                   # getting total number of frames from a video\n",
    "                total_frames_number = int(total_frames_number)                                           # changing float to integer   \n",
    "                frame_idxs_to_process = np.linspace(0, total_frames_number-1, frame_numbers, dtype=int)  # picking desiered frame indexes\n",
    "                \n",
    "                for idx in frame_idxs_to_process:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)                             # set the video to the desired frame index\n",
    "                    ret, frame= cap.read()                                            # reading the frame \n",
    "                    result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # processing the frame (Mediapipe works with RGB)\n",
    "                    pose,face,lh, rh= get_frame_detections(result)                    # turning results into flattened arrays\n",
    "                    frame_detection= np.concatenate((pose,face,lh, rh))  \n",
    "                    video_detections.append(frame_detection)                          # storing the frame detection in the video detection list\n",
    "                    \n",
    "                if class_names== autslclass_names:    # for AUTSL\n",
    "                    video_idx= int(os.path.basename(os.path.dirname(video_path))) # extract video index from the video folder\n",
    "                    label= vid_idx_to_label[video_idx]                            # map the video index to a label\n",
    "                    \n",
    "                elif class_names== lsa64class_names:  # for LSA64\n",
    "                    video_idx= int(os.path.basename(video_path).split('_')[0])    # extract index from the video title: 001_004_003 -> 1\n",
    "                    label= vid_idx_to_label[video_idx-1]                          # map the index to the correct label\n",
    "                \n",
    "                labels.append(label)\n",
    "                detections.append(video_detections) \n",
    "   \n",
    "            cap.release()\n",
    "        \n",
    "    return detections, labels, len(all_video_paths), none_cv2_video_paths\n",
    "\n",
    "#----------------------------------------------------------------Helping functions--------------------------------------------------------------------------\n",
    "def get_frame_detections(result):\n",
    "    '''\n",
    "    This function turns the result objects obtianed with mediapipe into flattened numpy arrays\n",
    "    '''\n",
    "    pose= np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4) \n",
    "    face= np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3) \n",
    "    lh= np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh= np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "    return pose, face, lh, rh\n",
    "    \n",
    "\n",
    "def get_frame_coordinates(result, frame):\n",
    "    '''\n",
    "    This function turns the result objects to a list of coordinates\n",
    "    '''\n",
    "    p_co= [(int(r.x * frame.shape[1]), int(r.y * frame.shape[0])) for r in result.pose_landmarks.landmark] if result.pose_landmarks else [(0,0)]*33 \n",
    "    f_co= [(int(r.x * frame.shape[1]), int(r.y * frame.shape[0])) for r in result.face_landmarks.landmark] if result.face_landmarks else [(0,0)]* 468 \n",
    "    l_co= [(int(r.x * frame.shape[1]), int(r.y * frame.shape[0])) for r in result.left_hand_landmarks.landmark] if result.left_hand_landmarks else [(0,0)]*21\n",
    "    r_co= [(int(r.x * frame.shape[1]), int(r.y * frame.shape[0])) for r in result.right_hand_landmarks.landmark] if result.right_hand_landmarks else [(0,0)]*21\n",
    "    return p_co, f_co, l_co, r_co\n",
    "\n",
    "#---------------------------------------------------------------Additional functions------------------------------------------------------------------------\n",
    "# (!!!!!Since WLASL 100 was excluded these functions are not used in the main ipynb files. !!!!) nevertheless they are working and were developed by me\n",
    "# function to get landmarks from WLASL100 dataset. \n",
    "def get_landmarks_WLASL100(root: str,\n",
    "                           class_names: List[str],\n",
    "                           frame_numbers: int):\n",
    "    \"\"\"\n",
    "    This function retrieves all video paths from the WLSA100 directory. Then the function analysis videos frame by frame and extract landmark. Since some of \n",
    "    the videos have faulty frames. it checks for before and after frames first. incase those are faulty as well it puts an empty list for that frame of the\n",
    "    video. Finally the function is able to assigne each video, an array of detected landmarks and a label.    \n",
    "    Args:\n",
    "        root: Path to video WLASL100 dataset directory.\n",
    "        class_names: List of all words in the dataset.\n",
    "        frame_numbers: number of frames we want to take from the entire video.\n",
    "   Returns:\n",
    "        detections, labels, len(all_video_paths),len(none_cv2_video_paths) where:\n",
    "        detections: is a list of all mediapipe landmarks that were detected from all videos.\n",
    "        labels: is a list of labels corresponding to each video detection.\n",
    "        len(all_video_paths): is the number of videos in the dataset.\n",
    "        none_cv2_video_paths\" is a list of videos that OpenCV was not able to open.       \n",
    "    Example use:\n",
    "        results= get_landmarks_WLASL100(root= root, class_names= class_names frame_numbers= 30)\n",
    "        detections, labels, num_all_videos, none_cv2_video_paths= results[0], results[1], results[2], results[3]\n",
    "    \"\"\"\n",
    "    labels= []                    # a list to store video labels\n",
    "    detections= []                # a list to store all video detections\n",
    "    none_cv2_video_paths= []      # a list to store video paths that cv2 can't capture\n",
    "    \n",
    "    all_video_paths= Path(root).glob(\"**/*.mp4\")                        # a list to store all video paths in the dataset\n",
    "    all_video_paths= [str(path) for path in all_video_paths]            # changing path objects to strings since natosrt works with strings\n",
    "    all_video_paths= natsorted(all_video_paths)                         # sorted\n",
    "    vid_idx_to_label= {i:label for i, label in enumerate(class_names)}  # this mapping is used to change the video titles to labels\n",
    "    \n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for video_path in tqdm(all_video_paths, desc=\"Processing videos\"):\n",
    "            cap = cv2.VideoCapture(video_path)              # capture each video using Opencv\n",
    "            if not cap.isOpened():                          # if OpenCV can't capture the video\n",
    "                none_cv2_video_paths.append(video_path)     # add the video path to none_cv2_video_paths list\n",
    "            else:\n",
    "                video_detections= []\n",
    "                total_frames_number= cap.get(cv2.CAP_PROP_FRAME_COUNT)                                     # getting total number of frames from a video\n",
    "                total_frames_number = int(total_frames_number)                                             # changing float to integer   \n",
    "                frame_idxs_to_process = np.linspace(0, total_frames_number - 1, frame_numbers, dtype= int) # picking desiered frame indexes\n",
    "                \n",
    "                for idx in frame_idxs_to_process:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx) # set the video to the desired frame index\n",
    "                    ret, frame= cap.read()                # read the frame\n",
    "                    if not ret:                           # if the frame was \"unreadable\".\n",
    "                        print(f\"Failed to grab frame {idx}, of video {video_path} of length {total_frames_number} frames. trying adjacent frames...\")\n",
    "                        cap.set(cv2.CAP_PROP_POS_FRAMES, idx - 1)      # set video to previous frame\n",
    "                        ret, frame = cap.read()                        # read the frame\n",
    "                        if not ret:                                    # if previous was also \"unreadable\"\n",
    "                            cap.set(cv2.CAP_PROP_POS_FRAMES, idx + 1)  # set the video to next frame\n",
    "                            ret, frame = cap.read()                    # read frame\n",
    "                            \n",
    "                    if not ret:                           # if the return value is still False\n",
    "                        print(f\"Unable to retrieve any frames around index {idx}, of video {video_path} of length {total_frames_number} frames.\")\n",
    "                        frame_detection= []               # we add empty detection that will be filled later, using interpolation\n",
    "                        video_detections.append(frame_detection)\n",
    "                        continue\n",
    "                                \n",
    "                    result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                    pose,face,lh, rh= get_frame_detections(result)       # turning results into flattened arrays\n",
    "                    frame_detection= np.concatenate((pose,face,lh, rh))   \n",
    "                    video_detections.append(frame_detection)             # storing the frame detection in the video detection list\n",
    "\n",
    "                video_idx= int(os.path.basename(os.path.dirname(video_path))) # extract video index from the folder\n",
    "                label= vid_idx_to_label[video_idx-1]                          # map the video index to a label\n",
    "                detections.append(video_detections)    \n",
    "                labels.append(label)\n",
    "       \n",
    "            cap.release()\n",
    "            \n",
    "        return detections, labels, len(all_video_paths), none_cv2_video_paths\n",
    "\n",
    "# function to interpolate two frames of a video and fill in the bad frame.\n",
    "def interpolate_frame_detections(most_recent_detection, next_coming_detection, alpha):\n",
    "    \"\"\"\n",
    "    Based on the value of most recent detection and next coming detection which are the frames before and after our faulty frame returns a landmark array for\n",
    "    the faulty frame.\n",
    "    Args:\n",
    "        most_recent_detection: landmarks detected in previous frame.\n",
    "        next_coming_detection: landmarks detected in the next frame.\n",
    "        alpha: interpolation factor. \n",
    "    Returns:\n",
    "        either: (1 - alpha) * most_recent_detection + alpha * next_coming_detection\n",
    "        or: next_coming_detection\n",
    "        or: most_recent_detection\n",
    "    Example use:\n",
    "        video_detection[i]= interpolate_frames(most_recent_detection, next_coming_detection, 0.5)\n",
    "    \"\"\"\n",
    "    if most_recent_detection is None and next_coming_detection is not None:             # first to nth frames are all corrupt\n",
    "        return next_coming_detection\n",
    "    elif most_recent_detection is not None and next_coming_detection is None:           # nth to last frames are all corrupt\n",
    "        return most_recent_detection\n",
    "    else:\n",
    "        return (1 - alpha) * most_recent_detection + alpha * next_coming_detection \n",
    "\n",
    "# function to fill the empty detections in the videos using interpolation\n",
    "def fill_empty_detections(detections):\n",
    "    \"\"\"\n",
    "    In principle fills up the empty landmark detections for frames that where faulty in the dataset and returns the dataset.\n",
    "    Args:\n",
    "        detections: all video detections from mediapipe\n",
    "    Returns:\n",
    "        detections (with no empty landmark frame)\n",
    "    Example use: \n",
    "        detections= fill_empty_detections(detections)\n",
    "    \"\"\"\n",
    "    for video_detection in detections:\n",
    "        most_recent_detection= None\n",
    "        for i in range(len(video_detection)):\n",
    "            if len(video_detection[i]) != 0:\n",
    "                most_recent_detection= video_detection[i]\n",
    "            else:\n",
    "                next_coming_detection= None\n",
    "                for j in range(i+1, len(video_detection)):\n",
    "                    if len(video_detection[j]) != 0:\n",
    "                        next_coming_detection= video_detection[j]\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                     \n",
    "                video_detection[i]= interpolate_frame_detections(most_recent_detection, next_coming_detection, 0.5)\n",
    "                most_recent_detection= video_detection[i]\n",
    "\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04e0b6-e0d7-4b7a-92eb-acfc5d03939b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making preprocess_utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "259a9e47-5778-4fdd-b2d7-266a491350b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/preprocess_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/preprocess_utils.py\n",
    "#Explanation: This python file contains functions for preprocessing our data.\n",
    "#------------------------------------------------------------------------------Import--------------------------------------------------------------------------\n",
    "\n",
    "# importing libraries for preprocessing data\n",
    "import random \n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing tqdm for progression bar and typing and numpy.typing for writing input types for each function\n",
    "from tqdm.auto import tqdm \n",
    "from typing import List, Tuple\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "#-----------------------------------------------------------------------Interpolation--------------------------------------------------------------------------\n",
    "# function to interpolate 2 video detections\n",
    "def interpolate_video_detections(video_detection_1: NDArray[np.float64], \n",
    "                                 video_detection_2: NDArray[np.float64], \n",
    "                                 frame_structure: List[Tuple[int, int]],\n",
    "                                 alpha: float):\n",
    "    \"\"\"\n",
    "    This function gets two video detection arrays and based interpolates them frame by frame. to make correct interpolations the function\n",
    "    first checks , if both frames contain same body parts.\n",
    "    Args:\n",
    "        video_detection_1: First video detection array.\n",
    "        video_detection_2: Second video detection array.\n",
    "        frame_structure: represents the start and end index for each landmark class: pose, face, lh, rh.\n",
    "        alpha: interpolation factor\n",
    "    Returns:\n",
    "        an array that is the interpolation of the two input video detections:\n",
    "        inter_vid_detection\n",
    "    Example usage: \n",
    "        inter_vid_detection = interpolate_video_detections(video_detection_1= v1, video_detection_2= v2, frame_structure= frame_structure, alpha= 0.5)\n",
    "    \"\"\"\n",
    "    num_frames = video_detection_1.shape[0]                # number of frames that will be interpolated\n",
    "    inter_vid_detection= np.zeros_like(video_detection_1)  # zero array for storing interpolated values\n",
    "    for i in range(num_frames):\n",
    "        frame_detection_1= video_detection_1[i]             \n",
    "        frame_detection_2= video_detection_2[i]             \n",
    "        inter_frame_detection= np.zeros_like(frame_detection_1) # stores interpolated frame\n",
    "        \n",
    "        for (start, end) in frame_structure:\n",
    "            bodypart1= frame_detection_1[start:end]    # body part in frame\n",
    "            bodypart2= frame_detection_2[start:end]    # body part in frame\n",
    "    \n",
    "            if np.all(bodypart1 == 0) and np.all(bodypart2 == 0):       # if the body part does not exist in both frames\n",
    "                inter_frame_detection[start:end] = np.zeros(end- start) # put zero    \n",
    "\n",
    "            elif np.all(bodypart1 == 0):                                # if body part 1 does not exist                   \n",
    "                inter_frame_detection[start:end] = bodypart2            # put bodypart 2\n",
    "            \n",
    "            elif np.all(bodypart2 == 0):                                # if body part 2 does not exist                     \n",
    "                inter_frame_detection[start:end] = bodypart1            # put bodypart 1\n",
    "            \n",
    "            else:  # if both exists then we interpolate\n",
    "                inter_frame_detection[start:end]= (1 - alpha) * bodypart1 + alpha * bodypart2\n",
    "                \n",
    "        inter_vid_detection[i]= inter_frame_detection \n",
    "    return inter_vid_detection\n",
    "\n",
    "# function to apply the interpolation to the entire dataset\n",
    "def interpolate_dataset(detections: NDArray[np.float64],\n",
    "                        labels: List[str],\n",
    "                         alpha: float= 0.5,\n",
    "                         noise_level: float= 0.001):\n",
    "    \"\"\"\n",
    "    This function applies interpolation accross the entire dataset. It only interpolates between videos that have the same label. \n",
    "    Args:\n",
    "        detections: array of all video detections from LSA64 or WLASL100 dataset\n",
    "        labels: list of all video labels in the dataset\n",
    "        alpha: interpolation factor.\n",
    "        num_interpolations_samples: number of interpolated samples that should be produced for each label\n",
    "    Returns:\n",
    "        a tuple of (np.array(x), y) where np.array(x) is the detections and y is the labels\n",
    "    Example usage:\n",
    "        detections, labels = interpolate_dataset(detections, labels, alpha= 0.5, min_interpolations= 13)\n",
    "    \"\"\"\n",
    "    current_data= defaultdict(list)                 # stores current data\n",
    "    interpolated_data= defaultdict(list)            # stores interpolated data\n",
    "    \n",
    "    frame_structure= [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)]  # represents the indexes of the concatenated pose, face, lh, rh\n",
    "    \n",
    "    x = []  #stores augmented detections\n",
    "    y = []  #stores augmented labels\n",
    "\n",
    "    # making a dictionary where key is label and value is list of all videos with same label\n",
    "    for idx, label in enumerate(labels):\n",
    "        current_data[label].append(detections[idx])\n",
    "\n",
    "    # for each label, finding all video pair combinations:\n",
    "    for label, video_detections in current_data.items():\n",
    "        pairs= []\n",
    "        for i in range(len(video_detections)):\n",
    "            for j in range(i+1, len(video_detections)):\n",
    "                pairs.append((i, j))\n",
    "        # randomly select a number of pairs equal to the number of samples that are available for that label\n",
    "        selected_pairs = random.sample(pairs, len(video_detections))\n",
    "        # interpolating the randomly selected pairs\n",
    "        for (i, j) in selected_pairs:\n",
    "            video_detection_1= video_detections[i]\n",
    "            video_detection_2= video_detections[j]\n",
    "            \n",
    "            inter_vid_detection = interpolate_video_detections(video_detection_1, video_detection_2, frame_structure, alpha) #interpolate\n",
    "            # adding random gaussian noise\n",
    "            noise = np.random.normal(0, noise_level, inter_vid_detection.shape[1:])  \n",
    "            noisy_interpolated = np.clip(inter_vid_detection + noise, 0.001, 0.999)\n",
    "            # adding the new sample under the label it belongs to\n",
    "            interpolated_data[label].append(noisy_interpolated)\n",
    "            \n",
    "    # add video detections of both current and interpolated data together \n",
    "    for label in current_data:\n",
    "        original_videos = current_data[label]  # Original samples\n",
    "        interpolated_videos = interpolated_data[label]  # Interpolated samples\n",
    "\n",
    "        combined_videos = original_videos + interpolated_videos\n",
    "        sampled_videos = random.sample(combined_videos, len(original_videos))  # Randomly pick samples so that the original number of samples is preserved\n",
    "\n",
    "        for video_detection in sampled_videos:\n",
    "            x.append(video_detection)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(x), y\n",
    "\n",
    "#-------------------------------------------------------------------------Split Data--------------------------------------------------------------------------\n",
    "#function to convert detections and labels to the right format for training\n",
    "def convert(detections: NDArray[np.float64],\n",
    "            labels: List[str],\n",
    "            class_names: List[str]):\n",
    "    \"\"\"\n",
    "    This function maps our Labels to numbers so that they are prepared for the training phase (ex: it maps the label \"Red\" to number 1). It also changes the\n",
    "    detections from float64 to float32. since float64 would generate errors when training.\n",
    "    Args:\n",
    "        detections: array of all video detections\n",
    "        label: labels for each video detection\n",
    "        class_names: list of all class names withing the dataset. it is used to make a dictionray that converts labels to numbers.\n",
    "    Returns:\n",
    "        a tuple of (X, y) where X is our features/ detections and has type tensor float 32 and y is our label and has type long.\n",
    "    Example use:\n",
    "        X, y= convert(detections= detections, labels= labels, class_names= wlasl100class_names)\n",
    "    \"\"\"\n",
    "    label_to_number= {label: num for num, label in enumerate(class_names)} # used for mapping the labels to numbers\n",
    "    X= torch.tensor(detections, dtype=torch.float32)\n",
    "    y= [label_to_number[label] for label in labels]                        # a list that has all the labels but in number format\n",
    "    y= torch.tensor(y, dtype=torch.long)    \n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# fuction that splits the dataset for training\n",
    "def split_dataset(detections: NDArray[np.float64],\n",
    "                  labels: List[str],\n",
    "                  class_names: List[str],\n",
    "                  test_size: float):\n",
    "    \"\"\"\n",
    "    This function splits the dataset and converts them so that they are suitable for training process. \n",
    "    Args:\n",
    "        detections: video detections for the entire dataset.\n",
    "        labels: list of all video labels for the entire dataset.\n",
    "        class_names: list of all class names in the dataset\n",
    "        test_size: determines how data should be splitted    \n",
    "    Returns:\n",
    "        a tuple of (X_train, X_test, y_train, y_test) \n",
    "    Example usage:\n",
    "        xtrain, xtest, ytrain, ytest= split_dataset(detections, labels, class_names, 0.2)\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(detections, labels, test_size= test_size, random_state= 42, stratify=labels)\n",
    "    X_train, y_train= convert(X_train, y_train, class_names)\n",
    "    X_test, y_test= convert(X_test, y_test, class_names)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e7a2b-e15f-4b3b-940f-55afbe116636",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e1b868-aa25-48a3-9a3a-aa6263fed902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/models.py\n",
    "# Explanation: this file contains classes that are used to make the LSTM and transformer model variations.\n",
    "#---------------------------------------------------------------------------------Import-----------------------------------------------------------------------\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import math\n",
    "# importing typing for writing function input types\n",
    "from typing import List, Callable\n",
    "\n",
    "#-----------------------------------------------------------------Functions for building transformer-----------------------------------------------------------\n",
    "#normal positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, seq_len):\n",
    "    super().__init__()\n",
    "\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(position*div_term)\n",
    "    pe[:, 1::2] = torch.cos(position*div_term)\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.pe[:, :x.shape[1]]\n",
    "\n",
    "#multihead attention layer\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super().__init__()\n",
    "    assert d_model % num_heads == 0, \"d_model should be divisible by num_heads\"\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = d_model // num_heads\n",
    "\n",
    "    self.w_q = nn.Linear(d_model, d_model)\n",
    "    self.w_k = nn.Linear(d_model, d_model)\n",
    "    self.w_v = nn.Linear(d_model, d_model)\n",
    "    self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def scaled_dot_product_attention(self, Q, K, V):\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    attn_probs = torch.softmax(attn_scores, dim=1)\n",
    "    output = torch.matmul(attn_probs, V)\n",
    "    return output\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "  def combine_heads(self, x):\n",
    "    batch_size, num_heads, seq_len, d_k = x.shape\n",
    "    return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "  def forward(self, Q, K, V):\n",
    "    Q = self.split_heads(self.w_q(Q))\n",
    "    K = self.split_heads(self.w_k(K))\n",
    "    V = self.split_heads(self.w_v(V))\n",
    "\n",
    "    attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
    "    output = self.w_o(self.combine_heads(attn_output))\n",
    "    return output\n",
    "\n",
    "# feed forward layer\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "  def __init__(self, d_model, d_ff):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(d_model, d_ff)\n",
    "    self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# encoder\n",
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "    super().__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    attn_output = self.self_attn(x, x, x)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x + self.dropout(ff_output))\n",
    "    return x\n",
    "\n",
    "#------------------------------------------------------------------Transformer Models---------------------------------------------------------------------------\n",
    "# encoder based transformer model for classification (This parent class has no positional encoding)\n",
    "# PE is added to the inherited classes so the code is more clean and clear to read \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, class_names: List[str], seq_len: int, d_model: int, nhead: int, d_ff: int = 2048, num_layers: int = 2, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Transformer model for sign language classification\n",
    "        Parameters:\n",
    "            class_names : list of all the classes in the dataset.\n",
    "            seq_len : length of input sequences-> corresponds to frame numbers in a video sample.\n",
    "            d_model : dimention of the model inputs (number of features).\n",
    "            nhead : the number of attention heads in the multi-head attention layer.\n",
    "            d_ff : the dimension of the feedforward network.\n",
    "            num_layers: the number of layers in the Transformer encoder. Default is 2.\n",
    "            dropout : the dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_type = 'transformer' # this is used in the training to save some of the resutls in the correct directory for the model\n",
    "        self.class_names = class_names\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, nhead, d_ff, dropout) for i in range(num_layers)])\n",
    "        self.classifier = nn.Linear(in_features=d_model, out_features=len(self.class_names))\n",
    "        \n",
    "    def forward(self, src: torch.Tensor):\n",
    "        output = src\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output = encoder_layer(output)\n",
    "            \n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "# encoder based transformer model for classification, with positional encoding\n",
    "class PETransformer(Transformer):\n",
    "    def __init__(self, class_names: List[str], seq_len: int, d_model: int, nhead: int, d_ff: int = 2048, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__(class_names, seq_len, d_model, nhead, d_ff, num_layers, dropout)\n",
    "        self.model_type = 'PEtransformer'\n",
    "        self.positional_encoding = PositionalEncoding(d_model, seq_len)\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        output = self.positional_encoding(src)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output = encoder_layer(output)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "# encoder based transformer model for classification, with a learnable parameter for positional encoding\n",
    "class ParamTransformer(Transformer):\n",
    "    def __init__(self, class_names: List[str], seq_len: int, d_model: int, nhead: int, d_ff: int = 2048, num_layers: int = 2, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Transformer model with learnable parameter as encoding\n",
    "        \"\"\"\n",
    "        super().__init__(class_names, seq_len, d_model, nhead, d_ff, num_layers, dropout)\n",
    "        self.model_type = 'paramtransformer'\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        output = src + self.positional_encoding\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output = encoder_layer(output)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "#----------------------------------------------------------------------------LSTM Model------------------------------------------------------------------------\n",
    "class LstmModel(nn.Module):\n",
    "    def __init__(self, class_names: List[str], input_size: int, hidden_size: int, num_layers: int= 1, activition: Callable= nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.model_type= 'lstm'\n",
    "        self.num_layers = num_layers\n",
    "        self.class_names= class_names\n",
    "        self.lstm_layers= nn.ModuleList()\n",
    "        self.lstm_layers.append(nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True))\n",
    "        \n",
    "        for i in range(1, num_layers):\n",
    "            self.lstm_layers.append(nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True))\n",
    "        \n",
    "        self.fc = nn.Linear(in_features= hidden_size, out_features= len(self.class_names))\n",
    "        self.activition = activition\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        for lstm in self.lstm_layers:\n",
    "            output, final_states = lstm(output)\n",
    "            output = self.activition(output)\n",
    "\n",
    "        output= self.fc(output[:,-1,:])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d19870-fe1e-46f4-b5da-fce81f868982",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making train_utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8675fe6a-7a6e-4f77-9409-879eeacff80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/train_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/train_utils.py\n",
    "#Explanation: This python file contains functions for implementing the training step\n",
    "#-------------------------------------------------------------------------Import-------------------------------------------------------------------------------\n",
    "\n",
    "#importing libraries for training models\n",
    "import torch  \n",
    "from torch.utils.data import DataLoader # for writing input types\n",
    "# importing tqdm for progression bar\n",
    "from tqdm.auto import tqdm \n",
    "# importing typing for writing input types for the functions\n",
    "from typing import Callable, List\n",
    "\n",
    "#----------------------------------------------------------------Functions for training a model----------------------------------------------------------------\n",
    "#function for resetting the model parameters if needed\n",
    "def reset_model_parameters(model):\n",
    "    for name, module in model.named_children():\n",
    "        if hasattr(module, 'reset_parameters'):\n",
    "            module.reset_parameters()\n",
    "            \n",
    "# function to calculate accuracy\n",
    "def accuracy_fn(y_logits: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    returns accuracy based on true and predicted label values\n",
    "    Args:\n",
    "        y_logits: torch tensor that represents model outputs\n",
    "        y: torch tensor that represents true output values\n",
    "    Returns:\n",
    "        accuracy\n",
    "    Example usage: \n",
    "        accuracy= accuracy_fn(y_logits, y)\n",
    "    \"\"\"\n",
    "    y_preds= torch.argmax(y_logits, 1)                 # gives the position --> label of the strongest prediction\n",
    "    corrects= (y_preds==y)                             # compare prediction with truth\n",
    "    accuracy= corrects.sum().item()/ corrects.shape[0] # number of true predictions / all predictions\n",
    "    return accuracy\n",
    "\n",
    "# function to train the model\n",
    "def train_model(num_epochs: int,\n",
    "                model: torch.nn.Module,\n",
    "                train_dataloader: DataLoader,\n",
    "                test_dataloader: DataLoader,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                device: torch.device):\n",
    "    \"\"\"\n",
    "    Trains a model on given train and test data. and returns avg loss and avg accruacies for each epoch.\n",
    "    Args:\n",
    "        num_epochs: number of times (epochs) the model is trained with the entire dataset\n",
    "        model: model object\n",
    "        train_dataloader: DataLoader object of train dataset\n",
    "        test_dataloader: DataLoader object of test dataset.\n",
    "        optimizer: optimizing entity that updates the weights of the model\n",
    "        loss_fn: function to calculate loss\n",
    "        device: Cuda or CPU\n",
    "    Returns:\n",
    "        A tuple of (train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds) where:\n",
    "        train_losses is a list that contains avg train loss of all batches, for every epoch.\n",
    "        test_losses is a list that contains avg test loss of all batches, for every epoch.\n",
    "        train_accuracies is a list that contains avg train accuracy of all batches, for every epoch.\n",
    "        test_accuracies is a list that contains avg test accuracy of all batches, for every epoch.\n",
    "        y_trues and y_preds are used to draw confusion matrix (they get overwritten in each epoch so in principle the last value of y_trues and y_preds is\n",
    "        returned).\n",
    "    Example usage: \n",
    "        results= train(num_epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, accuracy_fn, device)\n",
    "        train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds= results[0], results[1], results[2], results[3], results[4], results[5]\n",
    "    \"\"\"\n",
    "    \n",
    "    train_losses= []     \n",
    "    test_losses= []          \n",
    "    train_accuracies= []      \n",
    "    test_accuracies= []       \n",
    " \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Epoch\"):\n",
    "        model.train()\n",
    "        train_loss= [] # a list to store loss of every batch\n",
    "        train_acc= []  # a list to store acc of every batch\n",
    "\n",
    "        for X, y in train_dataloader:\n",
    "            # sending detections and labels to device\n",
    "            X= X.to(device) \n",
    "            y= y.to(device)\n",
    "\n",
    "            # train the model\n",
    "            optimizer.zero_grad()\n",
    "            y_logits = model(X)\n",
    "            loss = loss_fn(y_logits, y)        # batch loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracy= accuracy_fn(y_logits, y) # batch accuracy\n",
    "\n",
    "            #add loss and accuray of the batch to the list\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(accuracy)\n",
    "            \n",
    "        # adding average loss and accuracy for the epoch\n",
    "        train_losses.append(sum(train_loss) / len(train_loss))  \n",
    "        train_accuracies.append(sum(train_acc) / len(train_acc))\n",
    "    \n",
    "        model.eval()      # setting model to evaluation mode so no weights are changed\n",
    "\n",
    "        y_trues= []       \n",
    "        y_preds= []       \n",
    "        test_loss= []     # list to store loss of every batch\n",
    "        test_acc= []      # list to store accuracy of every batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in test_dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                y_logits = model(X)\n",
    "                loss = loss_fn(y_logits, y)        # test batch loss\n",
    "                accuracy= accuracy_fn(y_logits, y) # test batch accuracy\n",
    "                \n",
    "                test_loss.append(loss.item())\n",
    "                test_acc.append(accuracy)\n",
    "                y_pred= torch.argmax(y_logits, 1)                 # predicted labels\n",
    "                \n",
    "                y_trues.extend(y.flatten().cpu().numpy())          # Store true labels\n",
    "                y_preds.extend(y_pred.flatten().cpu().numpy())     # Store predictions\n",
    "                \n",
    "        test_losses.append(sum(test_loss) / len(test_loss))\n",
    "        test_accuracies.append(sum(test_acc) / len(test_acc))\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9ed51-697b-4fb9-b027-924d61df5ff6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making plot_utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f2b7170-b057-4fea-96fb-d3696297cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/plot_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/plot_utils.py\n",
    "# Explanation: This python file contains functions for plotting training results and other important data.\n",
    "#----------------------------------------------------------------------Import-----------------------------------------------------------------------------------\n",
    "\n",
    "# importing libraries for plotting data                                                 \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# for writing input types for the functions                                                                                \n",
    "from typing import List\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "#-----------------------------------------------------------------visualizing video detections-----------------------------------------------------------------\n",
    "# functions for drawing video landmarks\n",
    "def draw_circles(frame: np.ndarray,\n",
    "                 frame_detection: NDArray[np.float64],\n",
    "                 frame_structure: List[tuple]):\n",
    "    \"\"\"\n",
    "    This function draws circles on the frame based on x and y position of the landmark. it uses the frame_structure list to handle drawing pose landmarks since\n",
    "    unlike other landmarks they have x, y, z and \"visibility values\"\n",
    "    Args:\n",
    "        frame: represents frame that is shown\n",
    "        frame_detection: represents the coordinates of the landmarks in a frame that was processed by mediapipe.\n",
    "        frame_structure: a list that represents the start and end index for each landmark class: pose, face, lh, rh.\n",
    "    Returns:\n",
    "        manipulated frame \n",
    "    Example usage:\n",
    "        frame = draw_circles(frame, frame_detection, [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)])\n",
    "    \"\"\"\n",
    "    for (start, end) in frame_structure:                            # iterate through frame structure: pose, face, lh, rh\n",
    "        bodypart= frame_detection[start:end]               \n",
    "        if (start, end) == frame_structure[0]:                      # for the pose landmarks:\n",
    "            for i in range(0, len(bodypart), 4):                    # iterate through pose landmark\n",
    "                x, y = bodypart[i], bodypart[i+ 1]                  # getting x and y values for drawing the circles\n",
    "                px = int(x * frame.shape[1])                                                   \n",
    "                py = int(y * frame.shape[0])\n",
    "                cv2.circle(frame, (px, py), 3, (0, 255, 0), -1)     # plotting circles on the frame\n",
    "        else:\n",
    "            for i in range(0, len(bodypart), 3):\n",
    "                x, y = bodypart[i], bodypart[i+ 1]\n",
    "                px = int(x * frame.shape[1]) \n",
    "                py = int(y * frame.shape[0])\n",
    "                cv2.circle(frame, (px, py), 3, (0, 255, 0), -1)\n",
    "    return frame\n",
    "\n",
    "# function to show the vidoe detections\n",
    "def show_video_detections(video_detection: NDArray[np.float64],\n",
    "                          frame_structure: List[tuple] = [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)],\n",
    "                          height: int = 720,\n",
    "                          width: int = 1280):\n",
    "    \"\"\"\n",
    "    This function draws Mediapipe landmarks that were detected from a video. It uses a video_detection array that has x, y, z( and visibility for pose) values. \n",
    "    here we only focus on the (x,y) coordinates we do not draw in 3D (no z or visibility).\n",
    "    Args:\n",
    "        video_detection: an array that represents video detections \n",
    "        frame_structure: a list that represents the start and end index for each landmark class: pose, face, lh, rh.\n",
    "        height and width: dimentions of the video\n",
    "    \"\"\"\n",
    "    cv2.namedWindow(\"video detection\", cv2.WINDOW_NORMAL)                   # make a window\n",
    "    cv2.resizeWindow(\"video detection\", width= width, height= height)       # resize the window to desired hight and width\n",
    "    try:      # try to plot\n",
    "        for frame_detection in video_detection:\n",
    "            frame = np.zeros((height, width, 3), dtype=np.uint8)            # making empty black frame\n",
    "            frame = draw_circles(frame, frame_detection, frame_structure)   # drawing circles on the frame\n",
    "            cv2.imshow(\"video detection\", frame)\n",
    "            if cv2.waitKey(100) & 0xFF == 27:  #ESC key\n",
    "                break\n",
    "    finally:  # guarantees that destroyAllWindows() is executed at the end. even if there is error in try part\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# function to plot the video detection in one frame\n",
    "def plot_video_detection_as_MHI(video_detection: NDArray[np.float64],\n",
    "                                num_movements: int= 10,\n",
    "                                height: int= 720,\n",
    "                                width: int= 1280,\n",
    "                                save_path: str = None,\n",
    "                                frame_structure: List[tuple]= [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)]):\n",
    "    \"\"\"\n",
    "    This function plots the entire video in one single frame also refered to as motion history image. the lh, rh and pose each have different colors\n",
    "    Args:\n",
    "        video_detection: an array that represents video detections \n",
    "        frame_structure: a list that represents the start and end index for each landmark class: pose, face, lh, rh.\n",
    "    \"\"\"\n",
    "    pose_xy, lh_xy, rh_xy = [], [], []       # lists to store (x, y) values for pose, lh and rh \n",
    "    mean_lh_xy, mean_rh_xy= [], []\n",
    "    for frame_detection in video_detection:                                           \n",
    "        for (start, end) in frame_structure:\n",
    "            bodypart = frame_detection[start:end]\n",
    "            # for pose \n",
    "            if (start, end) == frame_structure[0]:              \n",
    "                frame_pose_xy= [(bodypart[i], bodypart[i+ 1]) for i in range(0, len(bodypart), 4)]  # get (x, y) from (x, y, z, vis) of each landmark\n",
    "                mean_lh_xy.append(tuple(np.mean([frame_pose_xy[15], frame_pose_xy[17], frame_pose_xy[19]], axis=0))) # a more stable mean\n",
    "                mean_rh_xy.append(tuple(np.mean([frame_pose_xy[16], frame_pose_xy[18], frame_pose_xy[20]], axis=0))) # a more stable mean\n",
    "                pose_xy.append(frame_pose_xy)\n",
    "            # for left hand\n",
    "            elif (start, end) == frame_structure[2]:         \n",
    "                frame_lh_xy= [(bodypart[i], bodypart[i+ 1]) for i in range(0, len(bodypart), 3)]    # get (x, y) from (x, y, z) of each landmark\n",
    "                lh_xy.append(frame_lh_xy)\n",
    "                #mean_lh_xy.append(tuple(np.mean(frame_lh_xy, axis= 0)))     # store one x mean, y mean tuple for the entire left hand in the frame\n",
    "            # for right hand\n",
    "            elif (start, end) == frame_structure[3]:         \n",
    "                frame_rh_xy= [(bodypart[i], bodypart[i+ 1]) for i in range(0, len(bodypart), 3)]    # get (x, y) from (x, y, z) of each landmark\n",
    "                rh_xy.append(frame_rh_xy)                                   \n",
    "                #mean_rh_xy.append(tuple(np.mean(frame_rh_xy, axis= 0)))    # store one x mean, y mean tuple for the entire right hand in the frame\n",
    "    \n",
    "    pose_colors, lh_colors, rh_colors= [], [], []        #lists to store shades of colors for pose, lh, rh\n",
    "    for i in range(len(video_detection)): \n",
    "        pose_colors.append(plt.cm.Blues(np.log1p(i) / np.log1p(len(video_detection))))    # shade of blue\n",
    "        lh_colors.append(plt.cm.Greens(np.log1p(i) / np.log1p(len(video_detection))))     # shade of green\n",
    "        rh_colors.append(plt.cm.Reds(np.log1p(i) / np.log1p(len(video_detection))))       # shade of red\n",
    "\n",
    "    # plotting video as motion histogram image (MHI)\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    for i in range(len(video_detection)):\n",
    "        plt.scatter(*zip(*pose_xy[i]), color=pose_colors[i], s=5)       # draw pose landmark with blue\n",
    "        plt.scatter(*zip(*lh_xy[i]), color=lh_colors[i], s=5)           # draw lh landmark with green\n",
    "        plt.scatter(*zip(*rh_xy[i]), color=rh_colors[i], s=5)           # draw rh landmark with red\n",
    "\n",
    "    #plotting lines to depict movement nicer\n",
    "    points_for_lh_line= [mean_lh_xy[i] for i in np.linspace(0, len(mean_lh_xy)- 1, num_movements, dtype=int)]\n",
    "    points_for_rh_line= [mean_rh_xy[i] for i in np.linspace(0, len(mean_rh_xy)- 1, num_movements, dtype=int)]\n",
    "\n",
    "    plt.plot(*zip(*points_for_lh_line), linewidth=4, color= 'green') \n",
    "    plt.plot(*zip(*points_for_rh_line), linewidth=4, color= 'red') \n",
    "\n",
    "    plt.gca().set_aspect(height/width)\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "#---------------------------------------------------------------Visualizing training results--------------------------------------------------------------------\n",
    "def plot_loss_accuracy(train_losses: List[float],\n",
    "                       test_losses: List[float],\n",
    "                       train_accuracies: List[float],\n",
    "                       test_accuracies: List[float],\n",
    "                       batch_size: int,\n",
    "                       save_path: str):\n",
    "    \"\"\"\n",
    "    Draws loss and accuracy of a training session.\n",
    "    Example usage:\n",
    "        plot_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, 64)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    # plotting loss\n",
    "    plt.subplot(1, 2, 1) \n",
    "    plt.plot(train_losses, label='Train Loss')      \n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title(f'Loss over Epochs(batch size= {batch_size}), Last Loss:{test_losses[-1]}') # writing the final loss value\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    # plotting accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(test_accuracies, label='Test Accuracy')\n",
    "    plt.title(f'Acc over Epochs(batch size= {batch_size}), Last Acc: {test_accuracies[-1]}') # writing the final accuracy\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_trues: List[int],\n",
    "                          y_preds: List[int],\n",
    "                          class_names: List[str],\n",
    "                          num_epochs: int,\n",
    "                          save_path: str):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix of a model using true values and model predictions.\n",
    "    Example usage:\n",
    "        plot_confusion_matrix(y_trues, y_preds, class_names, num_epochs)\n",
    "    \"\"\"\n",
    "    conf_matrix = confusion_matrix(y_trues, y_preds)\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix after {num_epochs} epoches')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# function for drawing in tensor board.\n",
    "def draw_in_tensorboard(train_losses: List[float],\n",
    "                        test_losses: List[float],\n",
    "                        train_accuracies: List[float], \n",
    "                        test_accuracies: List[float],  \n",
    "                        log_dir: str):\n",
    "    \"\"\"\n",
    "    Plots loss and accuracy of the training process in tensor board.\n",
    "    Example usage:\n",
    "        draw_in_tensorboard(train_losses, test_losses, train_accuracies, test_accuracies, save_directory)\n",
    "    \"\"\"\n",
    "    with SummaryWriter(log_dir= log_dir) as writer:\n",
    "        losses_and_accuracies= zip(train_losses, test_losses, train_accuracies, test_accuracies)\n",
    "        for epoch , (tr_losses, te_losses, tr_accs, te_accs) in enumerate(losses_and_accuracies):\n",
    "            writer.add_scalar('Loss/train', tr_losses, epoch)\n",
    "            writer.add_scalar('Loss/test', te_losses, epoch)\n",
    "            writer.add_scalar('Accuracy/train', tr_accs, epoch)\n",
    "            writer.add_scalar('Accuracy/test', te_accs, epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af4f3b-a537-4b28-b239-c60ae19ecfc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making train.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f3a475-52ac-4612-93c0-c603c493cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/train.py\n",
    "# Explanation: this python file carries out the training process. from making the datast to plotting the results.\n",
    "#-----------------------------------------------------------------------Import-------------------------------------------------------------------------------\n",
    "\n",
    "# importing numpy, torch, nn, typing: for writing input types for the functions\n",
    "import numpy as np \n",
    "from numpy.typing import NDArray\n",
    "from typing import Callable, List, Tuple, Literal\n",
    "import torch\n",
    "from torch import nn\n",
    "# from tqdm.auto import tqdm  \n",
    "\n",
    "import torch.optim as optim                   #optimizer\n",
    "from torch.utils.data import Dataset          # dataset calss\n",
    "from torch.utils.data import DataLoader       #data loader\n",
    "    \n",
    "from sklearn.utils import resample         # used for bootstrapping\n",
    "from sklearn.model_selection import KFold  # for K fold cross validation if necessary\n",
    "\n",
    "# connecting the steps\n",
    "from preprocess_utils import interpolate_dataset, split_dataset, convert\n",
    "from plot_utils import draw_in_tensorboard, plot_confusion_matrix, plot_loss_accuracy\n",
    "from train_utils import train_model, reset_model_parameters # for K fold cross validation if necessary\n",
    "\n",
    "#-------------------------------------------------------------Constant variables and classes-------------------------------------------------------------------\n",
    "# path to experiment directory: where the tensorboard files are saved (!!!!should be changed based on system file structure!!!!)\n",
    "experiment_dir= \"C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/experiment_results\"\n",
    "# path to the current directory incase we want to save some plotting pictures quickly (!!!!should be changed based on system file structure!!!!)\n",
    "current_dir= \"C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/notebooks\"\n",
    "# A simple dataset class from to CustomImageDataset example from pytorch.org\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label\n",
    "        \n",
    "#----------------------------------------------------------------------Train functions-------------------------------------------------------------------------\n",
    "# function to configure the training enviroment\n",
    "def configure(detections: NDArray[np.float64], \n",
    "              labels: List[str],\n",
    "              class_names: List[str],\n",
    "              test_size: float,\n",
    "              batch_size: int,\n",
    "              num_epochs: int,\n",
    "              model: torch.nn.Module,\n",
    "              lr: float,\n",
    "              device: torch.device,\n",
    "              quick_save: bool,\n",
    "              results_name: str,\n",
    "              dataset_name: Literal['LSA64', 'AUTSL40']):\n",
    "    \"\"\"\n",
    "    This function configures the training enviroment. The function first splits the datasets, then creates dataset and dataloader objects, following that \n",
    "    the functions sends the model to the device, it defines loss function and optimizer algorithim for the train process. Afterwards the model trains the model\n",
    "    and plots the results and saves them to the right directory.\n",
    "    Args:\n",
    "        detections: array of all video detections\n",
    "        labels: list of all video labels\n",
    "        class_names: a list containing unique class names in the dataset\n",
    "        batch_size: batch size\n",
    "        num_epochs: number of epochs\n",
    "        lr: learning rate\n",
    "        device: the device that we use for training (Cuda or CPU)\n",
    "        results_name: used to identify different training results\n",
    "        quick_save: a boolean for quick saving the plots in current dir\n",
    "        data_set_dir: The directory where the results are saved\n",
    "    \"\"\"\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(detections, labels, test_size= test_size, random_state= 42, stratify=labels)\n",
    "\n",
    "    X_train, X_test, y_train, y_test= split_dataset(detections, labels, class_names, test_size)  # split the dataset\n",
    "    train_dataset= CustomDataset(X_train, y_train)      # train_dataset\n",
    "    test_dataset= CustomDataset(X_test, y_test)         # test dataset\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size= batch_size, num_workers=0, shuffle=True) # train dataloader \n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size= batch_size, num_workers=0, shuffle=False)  # test dataloader\n",
    "    model= model.to(device)                            # sending model to device: CUDA or CPU     \n",
    "    loss_fn = nn.CrossEntropyLoss()                    # cross entropy for loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr= lr) # Adam optimizer\n",
    "\n",
    "    train_l, test_l, train_a, test_a, y_trues, y_preds = train_model(num_epochs,model, train_loader, test_loader, optimizer, loss_fn, device)  # train model\n",
    "\n",
    "    save_path= f\"{current_dir}/loss_acc.png\" if quick_save else None\n",
    "    plot_loss_accuracy(train_l, test_l, train_a, test_a, batch_size, save_path)  # loss acc\n",
    "    save_path= f\"{current_dir}/confmat.png\" if quick_save else None\n",
    "    plot_confusion_matrix(y_trues, y_preds, class_names, num_epochs, save_path)  # confusion matrix\n",
    "\n",
    "    log_dir =f'{experiment_dir}/{dataset_name}/{model.model_type}/runs/{results_name}/'         # directory for saving the tensorboard files\n",
    "    draw_in_tensorboard(train_l, test_l, train_a, test_a, log_dir)  # drawing in tensor board\n",
    "\n",
    "# function to configure the Kfold cross validation\n",
    "def configure_Kfold(detections: NDArray[np.float64], \n",
    "                    labels: List[str],\n",
    "                    class_names: List[str],\n",
    "                    n_splits: int,\n",
    "                    batch_size: int,\n",
    "                    num_epochs: int,\n",
    "                    model_class: torch.nn.Module,\n",
    "                    model_args: dict,\n",
    "                    lr: float,\n",
    "                    device: torch.device,\n",
    "                    quick_save: bool):\n",
    "    \"\"\"\n",
    "    This function configures the training enviroment for KFold cross validation. For each fold, The function first splits the datasets, then creates dataset\n",
    "    and dataloader objects, following that the functions sends the model to the device, it defines loss function and optimizer algorithim for the train \n",
    "    process. Afterwards the model trains the model and plots the results and saves them to the right directory. \n",
    "    Args:\n",
    "        detections: array of all video detections\n",
    "        labels: list of all video labels\n",
    "        class_names: a list containing unique class names in the dataset\n",
    "        n_splits: the number of folds that the data will be divided to\n",
    "        batch_size: batch size\n",
    "        num_epochs: number of epochs\n",
    "        lr: learning rate\n",
    "        device: Cuda or CPU\n",
    "        quick_save: a boolean for quick saving the plots in current dir\n",
    "    \"\"\"\n",
    "    X, y= convert(detections, labels, class_names) # converting detections and labels to the right format.\n",
    "    dataset= CustomDataset(X, y)                   # making dataset\n",
    "    kf= KFold(n_splits=n_splits, shuffle=True)     # making kfold object to split the dataset\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f\"Fold {fold + 1} ------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        model = model_class(**model_args).to(device)\n",
    "        train_loader = DataLoader(dataset=dataset, batch_size= batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_idx))  # train_dataloader\n",
    "        test_loader = DataLoader(dataset=dataset, batch_size= batch_size, sampler=torch.utils.data.SubsetRandomSampler(test_idx))    # test dataloader\n",
    "        loss_fn = nn.CrossEntropyLoss()                       # loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr= lr)    # optimizer\n",
    "\n",
    "        train_l, test_l, train_a, test_a, y_trues, y_preds = train_model(num_epochs,model, train_loader, test_loader, optimizer, loss_fn, device) # train model\n",
    "\n",
    "        save_path= f\"{current_dir}/{fold+1}.png\" if quick_save else None \n",
    "        plot_loss_accuracy(train_l, test_l, train_a, test_a, batch_size, save_path)    # loss acc\n",
    "        # save_path= f\"{current_dir}/c{fold+1}.png\" if quick_save else None\n",
    "        # plot_confusion_matrix(y_trues, y_preds, class_names, num_epochs, save_path)  # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f10d26-f5c4-4f83-9906-5f33a79495cd",
   "metadata": {},
   "source": [
    "# making analyse_layer.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9273380-4fb5-4756-884d-9b13b8f93896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/analyse_layer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/analyse_layer.py\n",
    "# Explanation: this python file contains functions for analysing layer attentions and drawing sailency maps.\n",
    "#------------------------------------------------------------------------Import--------------------------------------------------------------------------------\n",
    "import captum\n",
    "from captum.attr import Attribution\n",
    "from captum.attr import Saliency\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm.auto import tqdm \n",
    "from torch import Tensor\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from prepare_datasets import get_frame_detections, get_frame_coordinates, autslclass_names, lsa64class_names\n",
    "from preprocess_utils import convert\n",
    "\n",
    "#------------------------------------------------------------------------Constants-----------------------------------------------------------------------------\n",
    "# these parameters most only be used when the 468 extra face landmarks are not included !!!!\n",
    "pose_idx, lh_idx, rh_idx= list(range(0,33)), list(range(33, 54)), list(range(54, 75))    # indexes in pose, lh, rh\n",
    "pose_l_idx, pose_r_idx= [11, 13, 15, 17, 19, 21], [12, 14, 16, 18, 20, 22]               # indexes corresponding to right arm body and left arm \n",
    "pose_m_idx= [idx for idx in pose_idx if idx not in pose_r_idx and idx not in pose_l_idx] # rest of the indexes: legs, hips, etc\n",
    "reordered_idxs= pose_m_idx + pose_l_idx + lh_idx + pose_r_idx + rh_idx                   # so left arm lh and right arm rh are next to each other\n",
    "\n",
    "reordered_landmarks= [\"Nose\", \"Left Eye Inner\", \"Left Eye\", \"Left Eye Outer\", \"Right Eye Inner\", \"Right Eye\", \"Right Eye Outer\", \"Left Ear\", \"Right Ear\",\n",
    "                      \"Left Mouth Corner\", \"Right Mouth Corner\", \"Left Hip\", \"Right Hip\", \"Left Knee\", \"Right Knee\", \"Left Ankle\", \"Right Ankle\", \"Left Heel\",\n",
    "                      \"Right Heel\", \"Left Foot Index\", \"Right Foot Index\", \"Left Shoulder\", \"Left Elbow\", \"Left Wrist\", \"Left Pinky\", \"Left Index\", \n",
    "                      \"Left Thumb\", \"Left Wrist\", \"Left Thumb CMC\", \"Left Thumb MCP\", \"Left Thumb IP\", \"Left Thumb Tip\", \"Left Index MCP\", \"Left Index PIP\",\n",
    "                      \"Left Index DIP\", \"Left Index Tip\", \"Left Middle MCP\",  \"Left Middle PIP\", \"Left Middle DIP\", \"Left Middle Tip\", \"Left Ring MCP\",\n",
    "                      \"Left Ring PIP\", \"Left Ring DIP\", \"Left Ring Tip\", \"Left Pinky MCP\", \"Left Pinky PIP\", \"Left Pinky DIP\", \"Left Pinky Tip\", \n",
    "                      \"Right Shoulder\", \"Right Elbow\", \"Right Wrist\", \"Right Pinky\", \"Right Index\", \"Right Thumb\", \"Right Wrist\", \"Right Thumb CMC\", \n",
    "                      \"Right Thumb MCP\", \"Right Thumb IP\", \"Right Thumb Tip\", \"Right Index MCP\", \"Right Index PIP\", \"Right Index DIP\", \"Right Index Tip\",\n",
    "                      \"Right Middle MCP\", \"Right Middle PIP\", \"Right Middle DIP\", \"Right Middle Tip\",  \"Right Ring MCP\", \"Right Ring PIP\", \"Right Ring DIP\",\n",
    "                      \"Right Ring Tip\", \"Right Pinky MCP\", \"Right Pinky PIP\", \"Right Pinky DIP\", \"Right Pinky Tip\"]\n",
    "\n",
    "mp_holistic= mp.solutions.holistic         # mediapipe holistic model\n",
    "mp_drawing= mp.solutions.drawing_utils     # pre-made class that has functions for drawing media pipe result object\n",
    "\n",
    "#--------------------------------------------------------------plotting data on video sample-------------------------------------------------------------------\n",
    "def plot_attributions_on_video(video_path: str,\n",
    "                               model: torch.nn.Module,\n",
    "                               captum_method: Attribution,\n",
    "                               class_names: List[str],\n",
    "                               device: torch.device,\n",
    "                               frame_numbers: int = 30):\n",
    "    \"\"\"\n",
    "    This function plots the attribution values for a given video (from LSA64 and AUTSL) \n",
    "    Args:\n",
    "        video_path: Path to the video sample.\n",
    "        captum_method: ex:  Saliency\n",
    "        class_names: List of all words in the dataset from which we took that video.\n",
    "        device: The device that the computation is going to take place on (CPU and GPU)\n",
    "        frame_numbers: number of frames we want to take from the entire video.\n",
    "    Example usage:\n",
    "        result_objs, coordiantes, video_detection, label= get_landmarks_from_vid(video_path, class_name, 30) \n",
    "    \"\"\"\n",
    "    vid_idx_to_label= {i:label for i, label in enumerate(class_names)}          # this mapping is used to change the video titles to labels\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"ERROR in opening the video path{video_path}\")\n",
    "        return\n",
    "        \n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        try:\n",
    "            frames= []\n",
    "            result_objs= []        # stores mediapipe result objects from each frame of the video\n",
    "            video_detection= []    # stores the detections from each frame of the video\n",
    "            video_coordinates= []  # stores the coordinates of the detected landmarks\n",
    "            \n",
    "            total_frames_number = cap.get(cv2.CAP_PROP_FRAME_COUNT)                                 \n",
    "            total_frames_number= int(total_frames_number)\n",
    "            frame_idxs_to_process = np.linspace(0, total_frames_number-1, frame_numbers, dtype=int) # desired frame indexes\n",
    "            \n",
    "            for idx in frame_idxs_to_process:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)       # set cv2 to the desired index\n",
    "                ret, frame= cap.read()                      # process the frame in that index\n",
    "                if not ret:\n",
    "                    print(\"unreadble frame detected\")       # incase there is any unreadable frame\n",
    "                    break   \n",
    "\n",
    "                result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # processing frame with Mediapipe\n",
    "                pose,face,lh, rh= get_frame_detections(result)\n",
    "                frame_detection= np.concatenate((pose, lh, rh))                   # here we eliminate face landmarks for drawing.\n",
    "                p_co, f_co, l_co, r_co= get_frame_coordinates(result, frame)      # turning results into coordinates\n",
    "                frame_coordinates= p_co+ l_co+ r_co                               # shape: 1, frame_number, 75 tuples                 \n",
    "                \n",
    "                frames.append(frame)                                              # append\n",
    "                result_objs.append(result)                                       \n",
    "                video_detection.append(frame_detection)  \n",
    "                video_coordinates.append(frame_coordinates)\n",
    "                    \n",
    "            if class_names== autslclass_names:    # for AUTSL\n",
    "                video_idx= int(os.path.basename(os.path.dirname(video_path))) # extract video index from the video folder\n",
    "                label= vid_idx_to_label[video_idx]                            # map the video index to a label      \n",
    "            elif class_names== lsa64class_names:  # for LSA64\n",
    "                video_idx= int(os.path.basename(video_path).split('_')[0])    # extract index from the video title: 001_004_003 -> 1\n",
    "                label= vid_idx_to_label[video_idx-1]                          # map the index to the correct label\n",
    "\n",
    "            print(f\"Calculating landmark attributions for the sign language video {label}\")\n",
    "            video_detection = np.array(video_detection, dtype=np.float64)                     # torch said to change list of numpy to np array so its faster\n",
    "            video_detection, label= convert(video_detection, [label], class_names)                                     # converting to tensors\n",
    "            attributions= landmark_attributions(model, captum_method, video_detection, label, device)                  # shape: 1, frame_number, 75\n",
    "\n",
    "            vid_color_scale= (attributions[0] - attributions[0].min()) / (attributions[0].max()-attributions[0].min()) # normalizing attributions\n",
    "            vid_color_scale= vid_color_scale* 255 \n",
    "            vid_color_scale = vid_color_scale.cpu().numpy()   \n",
    "\n",
    "            for i, frame in enumerate(frames):\n",
    "                plot_mp_landmarks(frame, result_objs[i])\n",
    "                plot_circle(frame, video_coordinates[i], vid_color_scale[i])\n",
    "                \n",
    "                cv2.imshow('Frame with Attributions', frame)\n",
    "                #cv2.resizeWindow('Frame with Attributions', int(frame.shape[1] * 0.6), int(frame.shape[0] * 0.6))  # make it smaller\n",
    "                if cv2.waitKey(30) & 0xFF == 27:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error happened while processing: {e}\")\n",
    "            raise\n",
    "            \n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "    \n",
    "#------------------------------------------------------------------working with attention values-------------------------------------------------------------\n",
    "# function for calculating mean attentions\n",
    "def landmark_attributions(model: torch.nn.Module,\n",
    "                          captum_method: Attribution,\n",
    "                          video_detection: torch.Tensor, \n",
    "                          label: torch.Tensor, \n",
    "                          device: torch.device,):\n",
    "    \"\"\"\n",
    "    Calculates the mean layer attribution of landmarks in  a video. each landmark has x, y, z (and in case of pose_landmarks visibility) values. mean layer\n",
    "    attribution\n",
    "    acts as a parameter that shows how much a landmark is effecting the output of the model.\n",
    "    Args:\n",
    "        model: model that we want to analyse\n",
    "        captum_method: LayerConductance, Saliency, IntegratedGradients\n",
    "        video detection : a tensor of shape(frame_number, 1662 or 258)\n",
    "        label: label of the video\n",
    "         device: The device that the computation is going to take place on (CPU and GPU)\n",
    "    Example usage:\n",
    "        lm_attributions= landmark_attributions(model,  sailency, video_detection, label, device)\n",
    "    \"\"\"\n",
    "    model.eval()                                                             # set model to evaluation mode\n",
    "    model.to(device)\n",
    "    video_detection, label = video_detection.to(device), label.to(device)    # put on GPU\n",
    "    video_detection= video_detection.unsqueeze(0)                        \n",
    "    video_detection.requires_grad_()\n",
    "\n",
    "    attributions= captum_method.attribute(inputs= video_detection, target= label.item())\n",
    "    \n",
    "    pose = attributions[:, :, :132]                                          # shape: 1, frame_number, 132\n",
    "    pose = pose.reshape(attributions.shape[0], attributions.shape[1], -1, 4) # shape: 1, frame_number, 33, 4\n",
    "    pose_means = pose.mean(dim=3)                                            # shape: 1, frame_number, 33\n",
    "    \n",
    "    rest = attributions[:, :, 132:]                                          # shape: 1, frame_number, 1530 or 126\n",
    "    rest = rest.reshape(attributions.shape[0], attributions.shape[1], -1, 3) # shape: 1, frame_number, 510 or 42, 3\n",
    "    rest_means = rest.mean(dim=3)                                            # shape: 1, frame_number, 510 or 42\n",
    "    means = torch.cat((pose_means, rest_means), dim=2)                       # shape: 1, frame_number, 510+33 or 42+33\n",
    "    \n",
    "    return means\n",
    "\n",
    "def landmark_attributions_for_dataset(model, captum_method, dataset, device):\n",
    "    '''\n",
    "    This function calculates the mean layer attribution of landmarks over the dataset.\n",
    "    Example Usage:\n",
    "        lm_atts_dataset= landmark_attributions_for_dataset(model, sailency, dataset, device)\n",
    "    '''\n",
    "    total_lm_atts= None\n",
    "    for data in tqdm(dataset):\n",
    "        video_detection, label = data[0], data[1]\n",
    "        video_lm_atts= landmark_attributions(model, captum_method, video_detection, label, device)\n",
    "    \n",
    "        if total_lm_atts is None:\n",
    "            total_lm_atts = torch.zeros_like(video_lm_atts)\n",
    "    \n",
    "        total_lm_atts+= video_lm_atts\n",
    "    \n",
    "    lm_atts_dataset = total_lm_atts / len(dataset)\n",
    "    return lm_atts_dataset\n",
    "#-------------------------------------------------------------------plotting heatmap -------------------------------------------------------------------------\n",
    "\n",
    "# function for drawing the attention heatmap\n",
    "def plot_atts_heatmap(attributions: Tensor, save_path: str, show_landmark_names: bool = False, vmin= None, vmax= None):\n",
    "    \"\"\"\n",
    "    For a video, this function plots attribution as a heatmap where x axis are frames, y axis are landmarks and the colors represent attribution values.\n",
    "    Args:\n",
    "        attributions: a tensor of shape(1, frame_number, 1662 or 258)\n",
    "        show_landmark_names: boolean variable for showing landmark names\n",
    "    Note: vmin and vmax are chosen based on trail and error.\n",
    "    \"\"\"\n",
    "    atts = attributions.detach().cpu().numpy()     # moving tensor to CPU for drawing \n",
    "    num_frames, num_features = atts[0].shape       # remove batch dimention and get frame and featur numbers\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    if show_landmark_names:\n",
    "        atts = atts[:, :, reordered_idxs]          # reorder the landmarks\n",
    "        y_ticks_labels = reordered_landmarks\n",
    "        y_ticks_positions = range(0, num_features)\n",
    "    else:\n",
    "        y_ticks_labels = None \n",
    "        y_ticks_positions = range(0, num_features, 5) \n",
    "        \n",
    "    plt.imshow(atts[0].T, cmap='viridis', aspect='auto', origin='lower', vmin= vmin, vmax= vmax) # we transpose attributions so landmarks are on y axis\n",
    "    plt.colorbar()\n",
    "    plt.xlim(0, num_frames - 1)\n",
    "    plt.xticks(range(0, num_frames))\n",
    "    plt.xlabel(\"frames\")\n",
    "    plt.ylim(0, num_features - 1)\n",
    "    plt.yticks(y_ticks_positions, y_ticks_labels)\n",
    "    plt.ylabel(\"features\")\n",
    "    plt.title(\"Attributions\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#---------------------------------------------------------------------plotting on data on video---------------------------------------------------------------\n",
    "def plot_mp_landmarks(frame, result):\n",
    "    \"\"\"\n",
    "    This function draws landmarks and connections on a given frame.\n",
    "    Args:\n",
    "        frame: video frame that we want to draw on.\n",
    "        result: the detected media pipe object corresponding to the frame.\n",
    "        \n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2),\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2))\n",
    "    mp_drawing.draw_landmarks(frame, result.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2),\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2))\n",
    "    mp_drawing.draw_landmarks(frame, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2),\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2))\n",
    "    mp_drawing.draw_landmarks(frame, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2),\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2))\n",
    "\n",
    "\n",
    "def plot_circle(frame, coordinates, frame_colorscale):\n",
    "    \"\"\"\n",
    "    This function visualizes layer attributions by drawing circles on detected landmarks.\n",
    "    Args:\n",
    "        frame: The video frame we want to draw on.\n",
    "        coordinates: List of (x, y) coordinates of landmarks detected in the frame.\n",
    "        frame_colorscale: a color scaling based on attribution values.\n",
    "                      \n",
    "    \"\"\"\n",
    "    for idx, color_scale in enumerate(frame_colorscale):  \n",
    "        intensity = int(color_scale)\n",
    "        color = (intensity, 255 , 0)\n",
    "        x, y = coordinates[idx]\n",
    "        cv2.circle(frame, (x, y), radius=5, color=color, thickness=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba34a1-be16-445a-9e1d-8a60676d4798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
