{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa887d6-34db-49b7-af91-485398946b5d",
   "metadata": {},
   "source": [
    "This *.ipynb* file is used to \n",
    "1. create\n",
    "2. modify\n",
    "3. manage\n",
    "\n",
    "the *.py* files within the /src directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb828fd-0275-404b-abeb-ba99cd2203ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_directory: C:\\Users\\sadeg\\OneDrive\\Desktop\\Thesis\\python_codes\\SignLanguageProject\n",
      "src_directory: C:\\Users\\sadeg\\OneDrive\\Desktop\\Thesis\\python_codes\\SignLanguageProject\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# defining paths\n",
    "root_dir= os.path.dirname(os.getcwd())\n",
    "src_dir= os.path.join(root_dir, 'src')\n",
    "data_dir= os.path.join(root_dir, 'data')\n",
    "\n",
    "print(f'project_directory: {root_dir}')\n",
    "print(f'src_directory: {src_dir}')\n",
    "\n",
    "src_dir= src_dir.replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536632e-c5cd-4c84-9450-d7803d4fb334",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making \\_\\_init\\_\\_.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a509c252-cb5d-4350-9bee-d8b82f0311a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/__init__.py\n",
    "# Explanation: This file will mark the source directory as a python package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb9160-81a2-453a-a835-5752cf9c274a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making prepare\\_datasets.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9b657d7-7a72-492d-bfbb-230982e507a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/prepare_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/prepare_datasets.py\n",
    "# Explanation: This python file will contain functions that are used to extract landmarks from LSA64 and WLASL100 datasets and prepare data.\n",
    "# importing libraries for working with directories of the libreries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from natsort import natsorted \n",
    "\n",
    "# importing OpenCV and Mediapipe to read videos and extract landmarks\n",
    "import cv2                                                                         \n",
    "import mediapipe as mp  \n",
    "\n",
    "# importing numpy to work with arrays\n",
    "import numpy as np                                                                \n",
    "                                                            \n",
    "# importing tqdm for progression bar and typing for writing input types for each function                                                      \n",
    "from tqdm.auto import tqdm                                                         \n",
    "from typing import Callable, List\n",
    "\n",
    "# A list for all class names in WLALS100\n",
    "wlasl100class_names= [\"accident\", \"africa\", \"all\", \"apple\", \"basketball\", \"bed\", \"before\", \"bird\", \"birthday\",\n",
    "                      \"black\", \"blue\", \"bowling\", \"brown\", \"but\", \"can\", \"candy\", \"chair\", \"change\", \"cheat\", \"city\",\n",
    "                      \"clothes\", \"color\", \"computer\", \"cook\", \"cool\", \"corn\", \"cousin\", \"cow\", \"dance\", \"dark\",\n",
    "                      \"deaf\", \"decide\", \"doctor\", \"dog\", \"drink\", \"eat\", \"enjoy\", \"family\", \"fine\", \"finish\",\n",
    "                      \"fish\", \"forget\", \"full\", \"give\", \"go\", \"graduate\", \"hat\", \"hearing\", \"help\", \"hot\",\n",
    "                      \"how\", \"jacket\", \"kiss\", \"language\", \"last\", \"letter\", \"like\", \"man\", \"many\", \"meet\",\n",
    "                      \"mother\", \"need\", \"no\", \"now\", \"orange\", \"paint\", \"paper\", \"pink\", \"pizza\", \"play\",\n",
    "                      \"pull\", \"purple\", \"right\", \"same\", \"school\", \"secretary\", \"shirt\", \"short\", \"son\", \"study\",\n",
    "                      \"table\", \"tall\", \"tell\", \"thanksgiving\", \"thin\", \"thursday\", \"time\", \"walk\", \"want\", \"what\",\n",
    "                      \"white\", \"who\", \"woman\", \"work\", \"wrong\", \"year\", \"yes\", \"book\", \"later\", \"medicine\"]\n",
    "\n",
    "# A list for all class names in LSA64\n",
    "lsa64class_names= ['Opaque', 'Red', 'Green', 'Yellow', 'Bright', 'Light-blue', 'Colors', 'Pink',\n",
    "                   'Women', 'Enemy', 'Son', 'Man', 'Away', 'Drawer', 'Born', 'Learn',\n",
    "                   'Call', 'Skimmer', 'Bitter', 'Sweet milk', 'Milk', 'Water', 'Food', 'Argentina',\n",
    "                   'Uruguay', 'Country', 'Last name', 'Where', 'Mock', 'Birthday', 'Breakfast', 'Photo',\n",
    "                   'Hungry', 'Map', 'Coin', 'Music', 'Ship', 'None', 'Name', 'Patience',\n",
    "                   'Perfume', 'Deaf', 'Trap', 'Rice', 'Barbecue', 'Candy', 'Chewing-gum', 'Spaghetti',\n",
    "                   'Yogurt', 'Accept', 'Thanks', 'Shut down', 'Appear', 'To land', 'Catch', 'Help',\n",
    "                   'Dance', 'Bathe', 'Buy', 'Copy', 'Run', 'Realize', 'Give', 'Find']\n",
    "\n",
    "#---------------------------------------------------------------LSA 64--------------------------------------------------------------------------\n",
    "# function to get landmarks from LSA64 dataset.\n",
    "def get_landmarks_LSA64(root: str,\n",
    "                        class_names: List[str],                    \n",
    "                        frame_numbers: int):\n",
    "    \"\"\"\n",
    "    This function initially retrieves all video paths from the LSA64 directory. Then uses a dictionary to map each class name in lsa64class_names \n",
    "    to numbers from 1 to 64, this is used later to detect correct label for each video. Then the function analysis videos frame by frame and extract\n",
    "    landmark. Finallythe function is able to assigne each video, an array of detected landmarks and a label.\n",
    "    Args:\n",
    "        root: Path to LSA64 video dataset directory.\n",
    "        class_names: List of all words in the dataset.\n",
    "        frame_numbers: number of frames we want to take from the each video in the dataset.\n",
    "    Returns:\n",
    "        A tuple of (detections, labels, len(all_video_paths),len(none_cv2_video_paths)) where:\n",
    "        detections is a list of all mediapipe landmarks that were detected from all videos.\n",
    "        labels is a list of labels corresponding to each video detection.\n",
    "        len(all_video_paths) is the number of videos in the dataset.\n",
    "        len(none_cv2_video_paths) is the number of videos that OpenCV cant capture.\n",
    "    Note:\n",
    "        video_detections has the following structure:\n",
    "        indexes (0 to 131) of the list correspond to the first 33 pose landmarks. each pose landmark has: x, y, z, visibility\n",
    "        indexes (132 to 1535) of the list correspond to the first 468 face landmarks. each face landmark has: x, y, z\n",
    "        indexes (1536 to 1598) of the list correspond to the first 21 left hand landmarks. each left hand landmark has x, y, z\n",
    "        indexes (1599 to 1661) of the list correspond to the first 21 right hand landmarks each right hand landmark has x, y, z\n",
    "        in total 1662 values that correspond to 543 landmark objects.\n",
    "    Example use:\n",
    "        results= get_landmarks_LSA64(root= root, class_names= lsa64class_names, frame_numbers= 30)\n",
    "        detections, labels, number_of_all_vidoes, number_of_bad_videos= results[0], results[1], results[2], results[3]\n",
    "    \"\"\"\n",
    "    labels= [] # a list to store video labels\n",
    "    detections= [] # a list to store all video detections\n",
    "   \n",
    "    none_cv2_video_paths= [] # a list to store video paths that cv2 can't capture\n",
    "    \n",
    "    all_video_paths= Path(root).glob(\"**/*.mp4\")             # a list to store all video paths in the dataset\n",
    "    all_video_paths= [str(path) for path in all_video_paths] # changing path objects to strings since natosrt works with strings\n",
    "    all_video_paths= natsorted(all_video_paths)              # sorted\n",
    "    \n",
    "    vid_idx_to_label= {i+1:label for i, label in enumerate(class_names)} # this mapping is used to change the video titles to labels\n",
    "    \n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for video_path in tqdm(all_video_paths, desc=\"Processing videos\"):\n",
    "            cap = cv2.VideoCapture(video_path)              # capture each video using OpenCV\n",
    "            if not cap.isOpened():                          # if OpenCV can't capture the video path\n",
    "                none_cv2_video_paths.append(video_path)     # add the video path to none_cv2_video_paths\n",
    "            else:                                           \n",
    "                video_detections= [] # a list to store video detections\n",
    "\n",
    "                total_frames_number= cap.get(cv2.CAP_PROP_FRAME_COUNT)                                  # getting total number of frames from a video\n",
    "                total_frames_number = int(total_frames_number)                                          # changing float to integer   \n",
    "                frame_idxs_to_process = np.linspace(0, total_frames_number-1, frame_numbers, dtype=int) # picking desiered frame indexes\n",
    "                \n",
    "                for idx in frame_idxs_to_process:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx) # set the video to the desired frame index\n",
    "                    ret, frame= cap.read()\n",
    "                    \n",
    "                    result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) # processing the frame (Mediapipe works with RGB...)\n",
    "                    pose= np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4) \n",
    "                    face= np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3) \n",
    "                    lh= np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "                    rh= np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "                    frame_detection= np.concatenate((pose,face,lh, rh)) # concatenating detected landmarks of the frame\n",
    "                    video_detections.append(frame_detection)            # storing the frame detection in the video detection list\n",
    "\n",
    "                video_idx= int(os.path.basename(video_path).split('_')[0]) # extract video index from the video title\n",
    "                label= vid_idx_to_label[video_idx]                         # map the video index to a label\n",
    "                \n",
    "                labels.append(label)\n",
    "                detections.append(video_detections) \n",
    "   \n",
    "            cap.release()\n",
    "        \n",
    "    return detections, labels, len(all_video_paths),len(none_cv2_video_paths)\n",
    "\n",
    "#---------------------------------------------------------------WLASL 100--------------------------------------------------------------------------\n",
    "# function to get landmarks from WLASL100 dataset.\n",
    "def get_landmarks_WLASL100(root: str,\n",
    "                           class_names: List[str],\n",
    "                           frame_numbers: int):\n",
    "    \"\"\"\n",
    "    This function initially retrieves all video paths from the WLSA100 directory. Then uses a dictionary to map each class name in wlasl100 \n",
    "    class_names to numbers from 1 to 100, this is used later to detect correct label for each video. Then the function analysis videos frame \n",
    "    by frame and extract landmark. Since some of the videos have faulty frames. it checks for before and after frames first. incase those are\n",
    "    faulty as well it puts an empty list for that frame of the video. Finally the function is able to assigne each video, \n",
    "    an array of detected landmarks and a label.    \n",
    "    Args:\n",
    "        root: Path to video dataset directory.\n",
    "        class_names: List of all words in the dataset.\n",
    "        frame_numbers: number of frames we want to take from the entire video.\n",
    "    Returns:\n",
    "        A tuple of (detections, labels, len(all_video_paths),len(none_cv2_video_paths)) where:\n",
    "        detections is a list of all mediapipe landmarks that were detected from all videos.\n",
    "        labels is a list of labels corresponding to each video detection.\n",
    "        len(all_video_paths) is the number of videos in the dataset.\n",
    "        len(none_cv2_video_paths) is the number of videos that OpenCV cant capture.\n",
    "    Note:\n",
    "        video_detections has the following structure:\n",
    "        indexes (0 to 131) of the list correspond to the first 33 pose landmarks. each pose landmark has: x, y, z, visibility\n",
    "        indexes (132 to 1535) of the list correspond to the first 468 face landmarks. each face landmark has: x, y, z\n",
    "        indexes (1536 to 1598) of the list correspond to the first 21 left hand landmarks. each left hand landmark has x, y, z\n",
    "        indexes (1599 to 1661) of the list correspond to the first 21 right hand landmarks each right hand landmark has x, y, z\n",
    "        in total 1662 values that correspond to 543 landmark objects.        \n",
    "    Example use:\n",
    "        results= get_landmarks_WLASL100(root= root, class_names= class_names frame_numbers= 30):\n",
    "    \"\"\"\n",
    "    labels= [] # a list to store video labels\n",
    "    detections= [] # a list to store all video detections\n",
    "   \n",
    "    none_cv2_video_paths= [] # a list to store video paths that cv2 can't capture\n",
    "    \n",
    "    all_video_paths= Path(root).glob(\"**/*.mp4\")             # a list to store all video paths in the dataset\n",
    "    all_video_paths= [str(path) for path in all_video_paths] # changing path objects to strings since natosrt works with strings\n",
    "    all_video_paths= natsorted(all_video_paths)              # sorted\n",
    "    \n",
    "    vid_idx_to_label= {i+1:label for i, label in enumerate(class_names)} # this mapping is used to change the video titles to labels\n",
    "    \n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for video_path in tqdm(all_video_paths, desc=\"Processing videos\"):\n",
    "            cap = cv2.VideoCapture(video_path)              # capture each video using Opencv\n",
    "            if not cap.isOpened():                          # if OpenCV can't capture the video\n",
    "                none_cv2_video_paths.append(video_path)     # add the video path to none_cv2_video_paths list\n",
    "            else:\n",
    "                video_detections= []\n",
    "                total_frames_number= cap.get(cv2.CAP_PROP_FRAME_COUNT)                                     # getting total number of frames from a video\n",
    "                total_frames_number = int(total_frames_number)                                             # changing float to integer   \n",
    "                frame_idxs_to_process = np.linspace(0, total_frames_number - 1, frame_numbers, dtype= int) # picking desiered frame indexes\n",
    "                \n",
    "                for idx in frame_idxs_to_process:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx) # set the video to the desired frame index\n",
    "                    ret, frame= cap.read()\n",
    "                    \n",
    "                    if not ret:\n",
    "                        # if the return value is False: meaning the frame was \"unreadable\".\n",
    "                        print(f\"Failed to grab frame {idx}, of video {video_path} of length {total_frames_number} frames. trying adjacent frames...\")\n",
    "                        cap.set(cv2.CAP_PROP_POS_FRAMES, idx - 1)\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            cap.set(cv2.CAP_PROP_POS_FRAMES, idx + 1)\n",
    "                            ret, frame = cap.read()\n",
    "                            \n",
    "                    if not ret:\n",
    "                        # if the return value is still False: meaning the next frame was also \"unreadable\"\n",
    "                        print(f\"Unable to retrieve any frames around index {idx}, of video {video_path} of length {total_frames_number} frames.\")\n",
    "                        frame_detection= [] # we add empty detection that will be filled later, using interpolation\n",
    "                        video_detections.append(frame_detection)\n",
    "                        continue\n",
    "                                \n",
    "                    result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                    pose= np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4) \n",
    "                    face= np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3) \n",
    "                    lh= np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "                    rh= np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "                    frame_detection= np.concatenate((pose,face,lh, rh)) # concatenating detected landmarks of the frame\n",
    "                    video_detections.append(frame_detection)            # storing the frame detection in the video detection list\n",
    "\n",
    "                video_idx= int(os.path.basename(os.path.dirname(video_path))) # extract video index from the video_path\n",
    "                label= vid_idx_to_label[video_idx]                            # map the video index to a label\n",
    "                detections.append(video_detections)    \n",
    "                labels.append(label)\n",
    "       \n",
    "            cap.release()\n",
    "            \n",
    "        return detections, labels, len(all_video_paths),len(none_cv2_video_paths)\n",
    "\n",
    "# function to interpolate two frames of a video.\n",
    "def interpolate_frames(most_recent_detection, next_coming_detection, alpha):\n",
    "    \"\"\"\n",
    "    Based on the value of most recent detection and next coming detection which are the frames before and after our faulty frame returns \n",
    "    a landmark array for the faulty frame.\n",
    "    Args:\n",
    "        most_recent_detection: landmarks detected in previous frame.\n",
    "        next_coming_detection: landmarks detected in the next frame.\n",
    "        alpha: interpolation factor. \n",
    "    Returns:\n",
    "        either: (1 - alpha) * most_recent_detection + alpha * next_coming_detection\n",
    "        or: next_coming_detection\n",
    "        or: most_recent_detection\n",
    "    Example use:\n",
    "        video_detection[i]= interpolate_frames(most_recent_detection, next_coming_detection, 0.5)\n",
    "    \"\"\"\n",
    "    if most_recent_detection is None and next_coming_detection is not None:             # first to nth frames are all corrupt\n",
    "        return next_coming_detection\n",
    "    elif most_recent_detection is not None and next_coming_detection is None:           # nth to last frames are all corrupt\n",
    "        return most_recent_detection\n",
    "    else:\n",
    "        return (1 - alpha) * most_recent_detection + alpha * next_coming_detection \n",
    "\n",
    "# function to fill the empty detections in the videos using interpolation\n",
    "def fill_empty_detections(detections):\n",
    "    \"\"\"\n",
    "    In principle fills up the empty landmark detections for frames that where faulty in the dataset and returns the dataset .\n",
    "    Args:\n",
    "        detections: all video detections from mediapipe\n",
    "    Returns:\n",
    "        detections (with no empty landmark frame)\n",
    "    Example use: \n",
    "        detections= fill_empty_detections(detections)\n",
    "    \"\"\"\n",
    "    for video_detection in detections:\n",
    "        most_recent_detection= None\n",
    "        for i in range(len(video_detection)):\n",
    "            if len(video_detection[i]) != 0:\n",
    "                most_recent_detection= video_detection[i]\n",
    "            else:\n",
    "                next_coming_detection= None\n",
    "                for j in range(i+1, len(video_detection)):\n",
    "                    if len(video_detection[j]) != 0:\n",
    "                        next_coming_detection= video_detection[j]\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                     \n",
    "                video_detection[i]= interpolate_frames(most_recent_detection, next_coming_detection, 0.5)\n",
    "                most_recent_detection= video_detection[i]\n",
    "\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04e0b6-e0d7-4b7a-92eb-acfc5d03939b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making preprocess_utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "259a9e47-5778-4fdd-b2d7-266a491350b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/preprocess_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/preprocess_utils.py\n",
    "#Explanation: This python file contains functions for preprocessing our data.\n",
    "# importing libraries for preprocessing data\n",
    "import random \n",
    "\n",
    "# importing numpy to work with arrays\n",
    "import numpy as np\n",
    "\n",
    "# import torch\n",
    "import torch\n",
    "\n",
    "# importing defaultdict for making dictionaries and sorting videos under labels\n",
    "from collections import defaultdict\n",
    "\n",
    "# importing train_test_split for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing tqdm for progression bar and typing and numpy.typing for writing input types for each function\n",
    "from tqdm.auto import tqdm \n",
    "from typing import List, Tuple\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "#---------------------------------------------------------------Interpolation--------------------------------------------------------------------------\n",
    "\n",
    "# function to implement interpolation on two videos\n",
    "def interpolate_video_detections(video_detection_1: NDArray[np.float64], \n",
    "                                 video_detection_2: NDArray[np.float64], \n",
    "                                 frame_structure: List[Tuple[int, int]],\n",
    "                                 alpha: float):\n",
    "    \"\"\"\n",
    "    This function gets two video detection arrays and based interpolates them frame by frame. to make correct interpolations the function\n",
    "    first checks , if both frames contain same body parts.\n",
    "    Args:\n",
    "        video_detection_1: First video detection array.\n",
    "        video_detection_2: Second video detection array.\n",
    "        frame_structure: represents the start and end index for each landmark class: pose, face, lh, rh.\n",
    "        alpha: interpolation factor\n",
    "    Returns:\n",
    "        an array that is the interpolation of the two input video detections:\n",
    "        inter_vid_detection\n",
    "    Example usage: \n",
    "        inter_vid_detection = interpolate_video_detections(video_detection_1= v1, video_detection_2= v2, frame_structure= frame_structure, alpha= 0.5)\n",
    "    \"\"\"\n",
    "    num_frames = video_detection_1.shape[0] # number of frames that will be interpolated\n",
    "    inter_vid_detection= np.zeros_like(video_detection_1) # zero array for storing interpolated values\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        frame_detection_1= video_detection_1[i]             \n",
    "        frame_detection_2= video_detection_2[i]             \n",
    "        inter_frame_detection= np.zeros_like(frame_detection_1) # stores interpolated frame\n",
    "        \n",
    "        for (start, end) in frame_structure:\n",
    "            bodypart1= frame_detection_1[start:end]\n",
    "            bodypart2= frame_detection_2[start:end]\n",
    "            # if the body part does not exist in both frames\n",
    "            if np.all(bodypart1 == 0) and np.all(bodypart2 == 0):\n",
    "                inter_frame_detection[start:end] = np.zeros(end- start) # put zero    \n",
    "            # if a body part 1 does not exist\n",
    "            elif np.all(bodypart1 == 0):                       \n",
    "                inter_frame_detection[start:end] = bodypart2 # put bodypart 2\n",
    "            # if body part 2 does not exist\n",
    "            elif np.all(bodypart2 == 0):                        \n",
    "                inter_frame_detection[start:end] = bodypart1 # put bodypart 1\n",
    "            # if both exists then we interpolate\n",
    "            else:\n",
    "                inter_frame_detection[start:end]= (1 - alpha) * bodypart1 + alpha * bodypart2\n",
    "                # this formula also works very nice\n",
    "                #A = bodypart1+ ((bodypart1 + bodypart2) / 2)**2 - (bodypart1)**2 + ((bodypart1 + bodypart2) / 2)**2 - (bodypart2)**2 \n",
    "                #B = bodypart1+ ((bodypart1 + bodypart2) / 2)**2 - (bodypart1)**2 + ((bodypart1 + bodypart2) / 2)**2 - (bodypart2)**2 \n",
    "                #inter_frame_detection[start:end]= (1 - alpha) * A + alpha * B\n",
    "\n",
    "        inter_vid_detection[i]= inter_frame_detection \n",
    "    return inter_vid_detection\n",
    "\n",
    "# function that applies interpolation accross the entire dataset\n",
    "def interpolate_dataset(detections: NDArray[np.float64],\n",
    "                        labels: List[str],\n",
    "                        alpha: float= 0.5,\n",
    "                        num_interpolation_samples: int= 10):\n",
    "    \"\"\"\n",
    "    This function applies interpolation accross the entire dataset. It only interpolates between videos that have the same label. \n",
    "    Args:\n",
    "        detections: array of all video detections from LSA64 or WLASL100 dataset\n",
    "        labels: list of all video labels in the dataset\n",
    "        alpha: interpolation factor.\n",
    "        num_interpolations_samples: number of interpolated samples that should be produced for each label\n",
    "    Returns:\n",
    "        a tuple of (np.array(x), y) where np.array(x) is the detections and y is the labels\n",
    "    Example usage:\n",
    "        detections, labels = interpolate_dataset(detections, labels, alpha= 0.5, min_interpolations= 13)\n",
    "    \"\"\"\n",
    "    current_data= defaultdict(list)                 # stores current data\n",
    "    interpolated_data= defaultdict(list)            # stores interpolated data\n",
    "    augumented_data = defaultdict(list)             # union of current and interpolated data\n",
    "    \n",
    "    frame_structure= [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)]  # represents the indexes of the concatenated pose, face, lh, rh\n",
    "    \n",
    "    x = []  #stores augmented detections\n",
    "    y = []  #stores augmented labels\n",
    "\n",
    "    # making a dictionary where key is label and value is list of all videos with same label\n",
    "    for idx, label in enumerate(labels):\n",
    "        current_data[label].append(detections[idx])\n",
    "\n",
    "    # for each label, finding all video pair combinations:\n",
    "    for label, video_detections in current_data.items():\n",
    "        pairs= []\n",
    "        for i in range(len(video_detections)):\n",
    "            for j in range(i+1, len(video_detections)):\n",
    "                pairs.append((i, j))\n",
    "\n",
    "        # since all considering all combinations is too much. randomly select a specific number of video pairs\n",
    "        selected_pairs = random.sample(pairs, min(num_interpolation_samples, len(pairs)))\n",
    "        for (i, j) in selected_pairs:\n",
    "            video_detection_1= video_detections[i]\n",
    "            video_detection_2= video_detections[j]\n",
    "            inter_vid_detection = interpolate_video_detections(video_detection_1, video_detection_2, frame_structure, alpha) #interpolate\n",
    "\n",
    "            # add to the interpolated_data dictionary\n",
    "            interpolated_data[label].append(inter_vid_detection)\n",
    "    \n",
    "    # add video detections of both current and interpolated data together \n",
    "    for data in (current_data, interpolated_data):\n",
    "        for label, video_detections in data.items():\n",
    "            augumented_data[label].extend(video_detections)\n",
    "    \n",
    "    # convert the dictionary back into detection, label arrays\n",
    "    for label, video_detections in augumented_data.items():\n",
    "        for video_detection in video_detections:\n",
    "            x.append(video_detection)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(x), y\n",
    "\n",
    "#---------------------------------------------------------------Split Data--------------------------------------------------------------------------\n",
    "\n",
    "def convert(detections: NDArray[np.float64],\n",
    "            labels: List[str],\n",
    "            class_names: List[str]):\n",
    "    \"\"\"\n",
    "    This function maps our Labels to numbers. so that they are prepared for the training phase. It also changes the detections\n",
    "    from float64 to float32. since float64 would generate errors when training.\n",
    "    Args:\n",
    "        detections: array of all video detections\n",
    "        label: labels for each video detection\n",
    "        class_names: list of all class names withing the dataset\n",
    "    Returns:\n",
    "        a tuple of (X, y) where X is our features/ detections and has type tensor float 32 and y is our label and has type long.\n",
    "    Example use:\n",
    "        X, y= convert(detections= detections, labels= labels, class_names= wlasl100class_names)\n",
    "    \"\"\"\n",
    "    label_to_number= {label: num for num, label in enumerate(class_names)} # used for mapping the labels to numbers\n",
    "    X= torch.tensor(detections, dtype=torch.float32)\n",
    "    y= [label_to_number[label] for label in labels] # a list that has all the labels but in number format\n",
    "    y= torch.tensor(y, dtype=torch.long)    \n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_dataset(detections: NDArray[np.float64],\n",
    "                  labels: List[str],\n",
    "                  class_names: List[str],\n",
    "                  test_size: float):\n",
    "    \"\"\"\n",
    "    This function splits the dataset and converts them so that they are suitable for training process (ex: it maps the label \"Red\" to number 1)\n",
    "    Args:\n",
    "        detections: video detections for the entire dataset.\n",
    "        labels: list of all video labels for the entire dataset.\n",
    "        class_names: list of all class names in the dataset\n",
    "        test_size: determines how data should be splitted    \n",
    "    Returns:\n",
    "        a tuple of (X_train, X_test, y_train, y_test) \n",
    "    Example usage:\n",
    "        xtrain, xtest, ytrain, ytest= split_dataset(detections, labels, class_names, 0.2)\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(detections, labels, test_size= test_size, random_state=42, stratify=labels)\n",
    "    X_train, y_train= convert(X_train, y_train, class_names)\n",
    "    X_test, y_test= convert(X_test, y_test, class_names)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e7a2b-e15f-4b3b-940f-55afbe116636",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e1b868-aa25-48a3-9a3a-aa6263fed902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/models.py\n",
    "# Explanation: this file contains classes that are used to make the LSTM and transformer model variations.\n",
    "# importing necessary libraries\n",
    "import torch \n",
    "from torch import nn\n",
    "import math\n",
    "# importing typing for writing function input types\n",
    "from typing import List, Callable\n",
    "\n",
    "#-----------------------------------------------------------------functions for building transformer-----------------------------------------------------------\n",
    "#normal positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, seq_len):\n",
    "    super().__init__()\n",
    "\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(position*div_term)\n",
    "    pe[:, 1::2] = torch.cos(position*div_term)\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.pe[:, :x.shape[1]]\n",
    "\n",
    "#multihead attention layer\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super().__init__()\n",
    "    assert d_model % num_heads == 0, \"d_model should be divisible by num_heads\"\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = d_model // num_heads\n",
    "\n",
    "    self.w_q = nn.Linear(d_model, d_model)\n",
    "    self.w_k = nn.Linear(d_model, d_model)\n",
    "    self.w_v = nn.Linear(d_model, d_model)\n",
    "    self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def scaled_dot_product_attention(self, Q, K, V):\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    attn_probs = torch.softmax(attn_scores, dim=1)\n",
    "    output = torch.matmul(attn_probs, V)\n",
    "    return output\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "  def combine_heads(self, x):\n",
    "    batch_size, num_heads, seq_len, d_k = x.shape\n",
    "    return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "  def forward(self, Q, K, V):\n",
    "    Q = self.split_heads(self.w_q(Q))\n",
    "    K = self.split_heads(self.w_k(K))\n",
    "    V = self.split_heads(self.w_v(V))\n",
    "\n",
    "    attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
    "    output = self.w_o(self.combine_heads(attn_output))\n",
    "    return output\n",
    "\n",
    "# feed forward layer\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "  def __init__(self, d_model, d_ff):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(d_model, d_ff)\n",
    "    self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# encoder\n",
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "    super().__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    attn_output = self.self_attn(x, x, x)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x + self.dropout(ff_output))\n",
    "    return x\n",
    "\n",
    "#------------------------------------------------------------------Transformer Models---------------------------------------------------------------------------\n",
    "# encoder based transformer model for classification, no positional encoding\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, class_names: List[str], seq_len: int, d_model: int, nhead: int, d_ff: int = 2048, num_layers: int = 2, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Transformer model for sign language classification\n",
    "        Parameters:\n",
    "            class_names : list of all the classes in the dataset.\n",
    "            seq_len : length of input sequences-> corresponds to frame numbers in a video sample.\n",
    "            d_model : dimention of the model inputs (number of features).\n",
    "            nhead : the number of attention heads in the multi-head attention layer.\n",
    "            d_ff : the dimension of the feedforward network.\n",
    "            num_layers: the number of layers in the Transformer encoder. Default is 2.\n",
    "            dropout : the dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_type = 'transformer'\n",
    "        self.class_names = class_names\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, nhead, d_ff, dropout) for i in range(num_layers)])\n",
    "        self.classifier = nn.Linear(in_features=d_model, out_features=len(self.class_names))\n",
    "        \n",
    "    def forward(self, src: torch.Tensor):\n",
    "        output = src\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output = encoder_layer(output)\n",
    "            \n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "# encoder based transformer model for classification, with positional encoding\n",
    "class PETransformer(Transformer):\n",
    "    def __init__(self, class_names: List[str], seq_len: int, d_model: int, nhead: int, d_ff: int = 2048, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__(class_names, seq_len, d_model, nhead, d_ff, num_layers, dropout)\n",
    "        self.model_type = 'PEtransformer'\n",
    "        self.positional_encoding = PositionalEncoding(d_model, seq_len)\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        output = self.positional_encoding(src)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output = encoder_layer(output)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "# encoder based transformer model for classification, with a learnable parameter for positional encoding\n",
    "class ParamTransformer(Transformer):\n",
    "    def __init__(self, class_names: List[str], seq_len: int, d_model: int, nhead: int, d_ff: int = 2048, num_layers: int = 2, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Transformer model with learnable parameter as encoding\n",
    "        \"\"\"\n",
    "        super().__init__(class_names, seq_len, d_model, nhead, d_ff, num_layers, dropout)\n",
    "        self.model_type = 'paramtransformer'\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        output = src + self.positional_encoding\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output = encoder_layer(output)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "# encoder based transformer model for classification, with 1D CNN for positional encoding\n",
    "class ConvoTransformer(Transformer):\n",
    "    def __init__(self, class_names: List[str], seq_len: int, d_model: int = 129, nhead: int = 3, d_ff: int = 2048, num_layers: int = 2, input_shape: int = 1662, kernel_size: int = 1, dropout: float = 0.1):\n",
    "        super().__init__(class_names, seq_len, d_model, nhead, d_ff, num_layers, dropout)\n",
    "        self.model_type = 'convotransformer'\n",
    "        self.positional_encoding = nn.Conv1d(in_channels=input_shape, out_channels=d_model, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        src = src.permute(0, 2, 1)\n",
    "        output = self.positional_encoding(src)\n",
    "        output = output.permute(0, 2, 1)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output = encoder_layer(output)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "#------------------------------------------------------------------------------------LSTM Model-----------------------------------------------------------------\n",
    "class LstmModel(nn.Module):\n",
    "    def __init__(self, class_names: List[str], input_size: int, hidden_size: int, num_layers: int= 1, activition: Callable= nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.model_type= 'lstm'\n",
    "        self.num_layers = num_layers\n",
    "        self.class_names= class_names\n",
    "        self.lstm_layers= nn.ModuleList()\n",
    "        self.lstm_layers.append(nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True))\n",
    "        \n",
    "        for _ in range(1, num_layers):\n",
    "            self.lstm_layers.append(nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True))\n",
    "        \n",
    "        self.fc = nn.Linear(in_features= hidden_size, out_features= len(self.class_names))\n",
    "        self.activition = activition\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        for lstm in self.lstm_layers:\n",
    "            output, hidden_states = lstm(output)\n",
    "            output = self.activition(output)\n",
    "\n",
    "        output= self.fc(output[:,-1,:])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d19870-fe1e-46f4-b5da-fce81f868982",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making train_utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8675fe6a-7a6e-4f77-9409-879eeacff80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/train_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/train_utils.py\n",
    "#Explanation: This python file contains functions for implementing the training step\n",
    "#importing libraries for training models\n",
    "import torch  \n",
    "from torch.utils.data import DataLoader # for writing input types\n",
    "# importing tqdm for progression bar\n",
    "from tqdm.auto import tqdm \n",
    "# importing typing for writing input types for the functions\n",
    "from typing import Callable, List\n",
    "\n",
    "#function for resetting the model parameters if needed\n",
    "def reset_model_parameters(model):\n",
    "    for name, module in model.named_children():\n",
    "        if hasattr(module, 'reset_parameters'):\n",
    "            module.reset_parameters()\n",
    "            \n",
    "# function to calculate accuracy\n",
    "def accuracy_fn(y_logits: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    returns accuracy based on true and predicted label values\n",
    "    Args:\n",
    "        y_logits: torch tensor that represents model outputs\n",
    "        y: torch tensor that represents true output values\n",
    "    Returns:\n",
    "        accuracy\n",
    "    Example usage: \n",
    "        accuracy= accuracy_fn(y_logits, y)\n",
    "    \"\"\"\n",
    "    y_preds= torch.argmax(y_logits, 1)                 # gives the position --> label of the strongest prediction\n",
    "    corrects= (y_preds==y)                             # compare prediction with truth\n",
    "    accuracy= corrects.sum().item()/ corrects.shape[0] # number of true predictions / all predictions\n",
    "    return accuracy\n",
    "\n",
    "# function to train the model\n",
    "def train_model(num_epochs: int,\n",
    "                model: torch.nn.Module,\n",
    "                train_dataloader: DataLoader,\n",
    "                test_dataloader: DataLoader,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                device: torch.device):\n",
    "    \"\"\"\n",
    "    Trains a model on given train and test data. and returns avg loss and avg accruacies for each epoch.\n",
    "    Args:\n",
    "        num_epochs: number of times (epochs) the model is trained with the entire dataset\n",
    "        model: model object\n",
    "        train_dataloader: DataLoader object of train dataset\n",
    "        test_dataloader: DataLoader object of test dataset.\n",
    "        optimizer: optimizing entity that updates the weights of the model\n",
    "        loss_fn: function to calculate loss\n",
    "        device: Cuda or CPU\n",
    "    Returns:\n",
    "        A tuple of (train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds) where:\n",
    "        train_losses is a list that contains avg train loss of all batches, for every epoch.\n",
    "        test_losses is a list that contains avg test loss of all batches, for every epoch.\n",
    "        train_accuracies is a list that contains avg train accuracy of all batches, for every epoch.\n",
    "        test_accuracies is a list that contains avg test accuracy of all batches, for every epoch.\n",
    "        y_trues and y_preds are used to draw confusion matrix (they get overwritten in each epoch so in principle the last value of y_trues and y_preds is\n",
    "        returned).\n",
    "    Example usage: \n",
    "        results= train(num_epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, accuracy_fn, device)\n",
    "        train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds= results[0], results[1], results[2], results[3], results[4], results[5]\n",
    "    \"\"\"\n",
    "    \n",
    "    train_losses= []     \n",
    "    test_losses= []          \n",
    "    train_accuracies= []      \n",
    "    test_accuracies= []       \n",
    " \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Epoch\"):\n",
    "        model.train()\n",
    "        train_loss= [] # a list to store loss of every batch\n",
    "        train_acc= []  # a list to store acc of every batch\n",
    "\n",
    "        for X, y in train_dataloader:\n",
    "            # sending detections and labels to device\n",
    "            X= X.to(device) \n",
    "            y= y.to(device)\n",
    "\n",
    "            # train the model\n",
    "            optimizer.zero_grad()\n",
    "            y_logits = model(X)\n",
    "            loss = loss_fn(y_logits, y)        # batch loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracy= accuracy_fn(y_logits, y) # batch accuracy\n",
    "\n",
    "            #add loss and accuray of the batch to the list\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(accuracy)\n",
    "            \n",
    "        # adding average loss and accuracy for the epoch\n",
    "        train_losses.append(sum(train_loss) / len(train_loss))  \n",
    "        train_accuracies.append(sum(train_acc) / len(train_acc))\n",
    "    \n",
    "        model.eval()      # setting model to evaluation mode so no weights are changed\n",
    "\n",
    "        y_trues= []       \n",
    "        y_preds= []       \n",
    "        test_loss= []     # list to store loss of every batch\n",
    "        test_acc= []      # list to store accuracy of every batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in test_dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                y_logits = model(X)\n",
    "                loss = loss_fn(y_logits, y)        # test batch loss\n",
    "                accuracy= accuracy_fn(y_logits, y) # test batch accuracy\n",
    "                \n",
    "                test_loss.append(loss.item())\n",
    "                test_acc.append(accuracy)\n",
    "                y_pred= torch.argmax(y_logits, 1)                 # predicted labels\n",
    "                \n",
    "                y_trues.extend(y.flatten().cpu().numpy())          # Store true labels\n",
    "                y_preds.extend(y_pred.flatten().cpu().numpy())     # Store predictions\n",
    "                \n",
    "        test_losses.append(sum(test_loss) / len(test_loss))\n",
    "        test_accuracies.append(sum(test_acc) / len(test_acc))\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9ed51-697b-4fb9-b027-924d61df5ff6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# making plot_utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f2b7170-b057-4fea-96fb-d3696297cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/plot_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/plot_utils.py\n",
    "# Explanation: This python file contains functions for plotting training results and other important data.\n",
    "# importing libraries for plotting data                                                 \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# for writing input types for the functions                                                                                \n",
    "from typing import List\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "#---------------------------------------------------------------Preprocessing-----------------------------------------------------------------------------------\n",
    "# functions for drawing video landmarks\n",
    "def draw_circles(frame: np.ndarray,\n",
    "                   frame_detection: NDArray[np.float64],\n",
    "                   frame_structure: List[tuple]):\n",
    "    \"\"\"\n",
    "    This function draws circles on the frame based on x and y position of the landmark.\n",
    "    Args:\n",
    "        frame: represents frame that is shown\n",
    "        frame_detection: represents the coordinates of the landmarks in a frame that was processed by mediapipe.\n",
    "        frame_structure: a list that represents the start and end index for each landmark class: pose, face, lh, rh.\n",
    "    Returns:\n",
    "        manipulated frame \n",
    "    Example usage:\n",
    "        frame = draw_circles(frame, frame_detection, [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)])\n",
    "    \"\"\"\n",
    "    for (start, end) in frame_structure:\n",
    "        bodypart= frame_detection[start:end]\n",
    "        if (start, end) == frame_structure[0]:\n",
    "            for i in range(0, len(bodypart), 4):\n",
    "                x, y = bodypart[i], bodypart[i+ 1]\n",
    "                px = int(x * frame.shape[1]) \n",
    "                py = int(y * frame.shape[0])\n",
    "                cv2.circle(frame, (px, py), 3, (0, 255, 0), -1)\n",
    "        else:\n",
    "            for i in range(0, len(bodypart), 3):\n",
    "                x, y = bodypart[i], bodypart[i+ 1]\n",
    "                px = int(x * frame.shape[1]) \n",
    "                py = int(y * frame.shape[0])\n",
    "                cv2.circle(frame, (px, py), 3, (0, 255, 0), -1)\n",
    "    return frame\n",
    "\n",
    "# function to show the vidoe detections\n",
    "def show_video_detections(video_detection: NDArray[np.float64]):\n",
    "    \"\"\"\n",
    "    This function draws Mediapipe landmarks that were detected from a video. It uses a video_detection array that has x, y, z( and visibility for pose) values. \n",
    "    here we only focus on the (x,y) coordinates we do not draw in 3D (no z or visibility).\n",
    "    Args:\n",
    "        video_detection: an array that represents video detections \n",
    "    \"\"\"\n",
    "    height, width= 720, 1280\n",
    "    frame_structure= [(0, 132), (132, 1536), (1536, 1599), (1599, 1662)]  \n",
    "    cv2.namedWindow(\"video detection\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"video detection\", width= width, height= height)\n",
    "    try:\n",
    "        for frame_detection in video_detection:\n",
    "            frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            frame = draw_circles(frame, frame_detection, frame_structure)\n",
    "            cv2.imshow(\"video detection\", frame)\n",
    "            if cv2.waitKey(100) & 0xFF == 27:  #ESC key\n",
    "                break\n",
    "    finally:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "#---------------------------------------------------------------Training-----------------------------------------------------------------------------------\n",
    "\n",
    "# function for drawing loss and accuracy of a training session\n",
    "def plot_loss_accuracy(train_losses: List[float],\n",
    "                       test_losses: List[float],\n",
    "                       train_accuracies: List[float],\n",
    "                       test_accuracies: List[float],\n",
    "                       batch_size: int):\n",
    "    \"\"\"\n",
    "    Draws loss and accuracy of a training session.\n",
    "    Args:\n",
    "        train_losses: list of train losses\n",
    "        test_losses: list of test losses\n",
    "        train_accuracies: list of train accuracies\n",
    "        test_accuracies: list of test accuracies\n",
    "        batch_size: batch size\n",
    "    Example usage:\n",
    "        plot_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, 64)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 9))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1) \n",
    "    plt.plot(train_losses, label='Train Loss')  \n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title(f'Loss over Epochs(batch size= {batch_size}), Last Loss:{test_losses[-1]}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(test_accuracies, label='Test Accuracy')\n",
    "    plt.title(f'Accuracy over Epochs(batch size= {batch_size}), Last Accuracy: {test_accuracies[-1]}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# function for drawing confusion matrix\n",
    "def plot_confusion_matrix(y_trues: List[int],\n",
    "                          y_preds: List[int],\n",
    "                          class_names: List[str],\n",
    "                          num_epochs: int):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix of a model using true values and model predictions.\n",
    "    Args:\n",
    "        y_trues: true values\n",
    "        y_preds: model predictions\n",
    "        class_names: list of all class names in the dataset\n",
    "        num_epochs: number of epochs\n",
    "    Example usage:\n",
    "        plot_confusion_matrix(y_trues, y_preds, class_names, num_epochs)\n",
    "    \"\"\"\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_trues, y_preds)\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    \n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    plt.title(f'Confusion Matrix after {num_epochs} epoches')\n",
    "    plt.show()\n",
    "\n",
    "# function to draw the training results in tensor board\n",
    "def draw_in_tensorboard(train_losses: List[float],\n",
    "                        test_losses: List[float],\n",
    "                        train_accuracies: List[float], \n",
    "                        test_accuracies: List[float],  \n",
    "                        save_directory: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plots loss and accuracy of the training process in tensor board.\n",
    "    Args:\n",
    "        train_losses: train loss values for all epochs\n",
    "        test_losses: test loss values for all epochs\n",
    "        train_accuracies: train accuracy values for all epochs\n",
    "        test_accuracies: test accuracy values for all epochs\n",
    "        save_directory: the directory in which the files need to be saved\n",
    "    Example usage:\n",
    "        draw_in_tensorboard(train_losses, test_losses, train_accuracies, test_accuracies, save_directory)\n",
    "    \"\"\"\n",
    "    \n",
    "    with SummaryWriter(log_dir= save_directory) as writer:\n",
    "        losses_and_accuracies= zip(train_losses, test_losses, train_accuracies, test_accuracies)\n",
    "        for epoch , (tr_losses, te_losses, tr_accs, te_accs) in enumerate(losses_and_accuracies):\n",
    "            writer.add_scalar('Loss/train', tr_losses, epoch)\n",
    "            writer.add_scalar('Loss/test', te_losses, epoch)\n",
    "            writer.add_scalar('Accuracy/train', tr_accs, epoch)\n",
    "            writer.add_scalar('Accuracy/test', te_accs, epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af4f3b-a537-4b28-b239-c60ae19ecfc6",
   "metadata": {},
   "source": [
    "# making train.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f3a475-52ac-4612-93c0-c603c493cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/train.py\n",
    "# Explanation: this python file carries out the training process. from making the datast to plotting the results.\n",
    "# importing numpy, torch, nn, typing: for writing input types for the functions\n",
    "import numpy as np \n",
    "from numpy.typing import NDArray\n",
    "from typing import Callable, List, Tuple, Literal\n",
    "import torch\n",
    "from torch import nn\n",
    "# from tqdm.auto import tqdm  \n",
    "\n",
    "import torch.optim as optim                   #optimizer\n",
    "from torch.utils.data import Dataset          # dataset calss\n",
    "from torch.utils.data import DataLoader       #data loader\n",
    "    \n",
    "from sklearn.utils import resample         # used for bootstrapping\n",
    "from sklearn.model_selection import KFold  # for K fold cross validation if necessary\n",
    "\n",
    "# connecting the steps\n",
    "from preprocess_utils import interpolate_dataset, split_dataset, convert\n",
    "from plot_utils import draw_in_tensorboard, plot_confusion_matrix, plot_loss_accuracy\n",
    "from train_utils import train_model, reset_model_parameters # for K fold cross validation if necessary\n",
    "\n",
    "# making a simple dataset class from to CustomImageDataset example from pytorch.org\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label\n",
    "#------------------------------------------------------------------train--------------------------------------------------------------------------------\n",
    "# function to configure the training enviroment\n",
    "def configure(detections: NDArray[np.float64], \n",
    "              labels: List[str],\n",
    "              class_names: List[str],\n",
    "              test_size: float,\n",
    "              batch_size: int,\n",
    "              num_epochs: int,\n",
    "              model: torch.nn.Module,\n",
    "              lr: float,\n",
    "              device: torch.device,\n",
    "              dir: Literal['LSA64', 'WLASL100']): # only allows on or the other\n",
    "    \"\"\"\n",
    "    This function configures the parameters parameters for the training of the model.  \n",
    "    Args:\n",
    "        detections: array of all video detections\n",
    "        labels: list of all video labels\n",
    "        class_names: a list containing unique class names in the dataset\n",
    "        batch_size: batch size\n",
    "        num_epochs: how many times to train the model\n",
    "        lr: determines the learning rate or rate with which we apply changes to model parameters\n",
    "        device: Cuda or CPU\n",
    "        dir: directory of experiment results -> LSA64 or  WLASL100\n",
    "    Returns:\n",
    "        trains the model and plots loss and accuracy for both train and test dataset in tensor board. also plots confusion matrix.\n",
    "    \"\"\"\n",
    "    save_directory =f'C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/experiment_results/{dir}/{model.model_type}/runs/'\n",
    "    X_train, X_test, y_train, y_test= split_dataset(detections, labels, class_names, test_size) # split the dataset\n",
    "    # making datasets and datloaders\n",
    "    train_dataset= CustomDataset(X_train, y_train)  \n",
    "    test_dataset= CustomDataset(X_test, y_test) \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size= batch_size, num_workers=0, shuffle=True) # train \n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size= batch_size, num_workers=0, shuffle=False)  # test\n",
    "    # send model to device: gpu or cpu\n",
    "    model= model.to(device)                  \n",
    "    loss_fn = nn.CrossEntropyLoss() #loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr= lr) #optimizer\n",
    "    #training the model\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds = train_model(num_epochs,model, train_loader, test_loader, optimizer, loss_fn, device)\n",
    "    # drawing the train results in tensorboard\n",
    "    draw_in_tensorboard(train_losses, test_losses, train_accuracies, test_accuracies, save_directory)\n",
    "    # draw confusion matrix\n",
    "    plot_confusion_matrix(y_trues, y_preds, class_names, num_epochs)\n",
    "\n",
    "#--------------------------------------------------------------------k fold------------------------------------------------------------------------------\n",
    "def configure_Kfold(detections: NDArray[np.float64], \n",
    "                    labels: List[str],\n",
    "                    class_names: List[str],\n",
    "                    n_splits: int,\n",
    "                    batch_size: int,\n",
    "                    num_epochs: int,\n",
    "                    model: torch.nn.Module,\n",
    "                    lr: float,\n",
    "                    device: torch.device):\n",
    "    \"\"\"\n",
    "    This function configures the parameters for K_fold_cross_validation.  \n",
    "    Args:\n",
    "        detections: array of all video detections\n",
    "        labels: list of all video labels\n",
    "        class_names: a list containing unique class names in the dataset\n",
    "        n_splits: determines the number of folds that the data will be divided to\n",
    "        batch_size: batch size\n",
    "        num_epochs: how many times to train the model\n",
    "        lr: determines the learning rate or rate with which we apply changes to model parameters\n",
    "        device: Cuda or CPU        \n",
    "    Returns:\n",
    "        plots loss and accuracy of both train and test dataset for each fold\n",
    "    \"\"\"\n",
    "    X, y= convert(detections, labels, class_names) # converting detections and labels to the right format.\n",
    "    dataset= CustomDataset(X, y)                   # making dataset\n",
    "    kf= KFold(n_splits=n_splits, shuffle=True)     # making kfold object to split the dataset\n",
    "    model= model.to(device)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "        print(\"------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Fold {fold + 1}\")\n",
    "        reset_model_parameters(model) # resetting the model parameters\n",
    "        # making dataloaders\n",
    "        train_loader = DataLoader(dataset=dataset, batch_size= batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_idx)) \n",
    "        test_loader = DataLoader(dataset=dataset, batch_size= batch_size, sampler=torch.utils.data.SubsetRandomSampler(test_idx))\n",
    "        # loss and optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr= lr)\n",
    "        # training the model\n",
    "        train_losses, test_losses, train_accuracies, test_accuracies, y_trues, y_preds = train_model(num_epochs,model, train_loader, test_loader, optimizer, loss_fn, device)\n",
    "        #plot results\n",
    "        plot_loss_accuracy(train_losses, test_losses, train_accuracies, test_accuracies, batch_size)\n",
    "        # Call the function to plot the confusion matrix\n",
    "        #plot_confusion_matrix(y_trues, y_preds, class_names, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f10d26-f5c4-4f83-9906-5f33a79495cd",
   "metadata": {},
   "source": [
    "# making analyse_layer.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9273380-4fb5-4756-884d-9b13b8f93896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/SignLanguageProject/src/analyse_layer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/analyse_layer.py\n",
    "# Explanation: this python file contains functions for analysing layer attentions\n",
    "# importing necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mp_holistic= mp.solutions.holistic     # mediapipe holistic model\n",
    "mp_drawing= mp.solutions.drawing_utils # pre-made class that has functions for drawing media pipe result object\n",
    "\n",
    "#--------------------------------------------------------------preparing data----------------------------------------------------------------------\n",
    "def get_landmarks_from_vid(video_path: str, frame_numbers: int = 30):\n",
    "    \"\"\"\n",
    "    This function returns a list of landmarks, a list of pixel coordinates(x,y) and a list of mediapipe result objects for a video path\n",
    "    by applying mediapipe model  to each frame of the video in LSA64 dataset. (can also be modified slightly to handle WLASL100 videos)\n",
    "    Args:\n",
    "        video_path: Path to video.\n",
    "        frame_numbers: number of frames we want to take from the entire video.\n",
    "    Returns:\n",
    "        A tuple of (results, coordinates, video_detections, label) where :\n",
    "        results is a list of mediapipe objects. It is used later for drawing mediapipe landmarks with mp_drawing class.\n",
    "        coordiantes is a list of (x, y) coordinates. It is used later for drawing circles with OpenCV, on specific landmarks.\n",
    "        video_detections is a flattened array of mediapipe detections. It is later fed to the model for layer analysis.\n",
    "        vid_index is an index number (range 0 to 63) that corresponds to the label of the vidoe.\n",
    "\n",
    "    Note:\n",
    "        video_detections has the following structure:\n",
    "        indexes (0 to 131) of the list correspond to the first 33 pose landmarks : x, y, z, visibility\n",
    "        indexes (132 to 1535) of the list correspond to the first 468 face landmarks: x, y, z\n",
    "        indexes (1536 to 1598) of the list correspond to the first 21 left hand landmarks: x, y, z\n",
    "        indexes (1599 to 1661) of the list correspond to the first 21 right hand landmarks: x, y, z\n",
    "    \"\"\"\n",
    "    \n",
    "    #'Example vidoe path: C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/lsa64_raw/all/001_001_001.mp4'\n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence= 0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened(): # if OpenCV and not read the video\n",
    "            print(f\"ERROR in opening the video path{video_path}\")    \n",
    "        else:\n",
    "            result, coordinates, video_detections= [], [], []\n",
    "            total_frames_number = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))                               \n",
    "            frame_idxs_to_process = np.linspace(0, total_frames_number-1, frame_numbers, dtype=int)  \n",
    "            \n",
    "            for idx in frame_idxs_to_process:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame= cap.read()\n",
    "                if not ret:\n",
    "                    print(\"unreadble frame detected\")\n",
    "                    break       \n",
    "                result= holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                results.append(result) # appending list of result objects\n",
    "                # making frame_detections by concatenating pose, face lh, rh \n",
    "                pose= np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4) \n",
    "                face= np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3) \n",
    "                lh= np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "                rh= np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "                frame_detection= np.concatenate((pose,face,lh, rh))\n",
    "                video_detections.append(frame_detection) # appending video detection\n",
    "                # storing coordinates\n",
    "                pose_coor= [(int(res.x * frame.shape[1]), int(res.y * frame.shape[0])) for res in result.pose_landmarks.landmark] if result.pose_landmarks else [(0, 0)] * 33 \n",
    "                face_coor= [(int(res.x * frame.shape[1]), int(res.y * frame.shape[0])) for res in result.face_landmarks.landmark] if result.face_landmarks else [(0, 0)] * 468 \n",
    "                lh_coor= [(int(res.x * frame.shape[1]), int(res.y * frame.shape[0])) for res in result.left_hand_landmarks.landmark] if result.left_hand_landmarks else [(0, 0)] * 21\n",
    "                rh_coor= [(int(res.x * frame.shape[1]), int(res.y * frame.shape[0])) for res in result.right_hand_landmarks.landmark] if result.right_hand_landmarks else [(0, 0)] * 21\n",
    "                coordinate= pose_coor+face_coor+lh_coor+rh_coor\n",
    "                coordinates.append(coordinate)\n",
    "                    \n",
    "            vid_index= int(os.path.basename(video_path).split('_')[0]) - 1\n",
    "        cap.release()\n",
    "        \n",
    "        return results, video_detections, coordinates, vid_index\n",
    "\n",
    "# function for calculating mean attentions\n",
    "def calculate_means(attributions: Tensor):\n",
    "    \"\"\"\n",
    "    Calculates the mean Layer attribution of a landmark. each landmark has x, y, z (and in case of pose_landmarks visibility) values\n",
    "    each landmark value has an attribution that can effect the transformer layer. mean acts as a parameter that shows how much a \n",
    "    landmark is effecting the output of the model.\n",
    "    Args:\n",
    "        attributions: a tensor of shape(1, frame_number, 1662)\n",
    "    Returns:\n",
    "        means: a tensor of shape (1, frame_number, 543)\n",
    "    \"\"\"\n",
    "    #pose -> x, y, z, vis\n",
    "    pose_part = attributions[:, :, :132]                                               #batch, seq_len, coordinates\n",
    "    pose_part = pose_part.reshape(attributions.shape[0], attributions.shape[1], -1, 4) #batch, seq_len, landmark, coordinates_per_landmark\n",
    "    pose_means = pose_part.mean(dim=3) # calculate mean for pose\n",
    "    # face, lh, rh -> x, y, z\n",
    "    rest = attributions[:, :, 132:] \n",
    "    rest = rest.reshape(attributions.shape[0], attributions.shape[1], -1, 3)\n",
    "    rest_means = rest.mean(dim=3) # mean for face, lh, rh\n",
    "    means = torch.cat((first_part_means, second_part_means), dim=2) # concatenate means: batch, seq_len, landmark, mean value\n",
    "    return means\n",
    "\n",
    "#-------------------------------------------------------------------plotting data ------------------------------------------------------------------\n",
    "\n",
    "# function for drawing the attention heatmap\n",
    "def plot_atts_heatmap(attributions: Tensor, title=\"Video\"):\n",
    "    \"\"\"\n",
    "    for each video, this function plots attribution as a heatmap where x axis are attributes and y axis are frames.\n",
    "    \"\"\"\n",
    "    vid_attributions = attributions.detach().cpu().numpy() # moving tensor to CPU for drawing \n",
    "    num_vids, num_frames, num_features = vid_attributions.shape\n",
    "    for num in range(num_vids): # for each video \n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.imshow(vid_attributions[num].T, cmap='cividis', aspect='auto', origin='lower') \n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.xlim(0, num_frames - 1)  # Set x-axis limits\n",
    "        plt.xticks(range(0, num_frames))  # Set ticks every 5 units\n",
    "        plt.xlabel(\"frames\")\n",
    "\n",
    "        plt.ylim(0, num_features - 1)  # Set y-axis limits\n",
    "        plt.yticks(range(0, num_features, 100))  # Set ticks every 10 units\n",
    "        plt.ylabel(\"features\")\n",
    "\n",
    "        plt.title(f\"{title} - Sample {num}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------plotting on data on video---------------------------------------------------------\n",
    "# I have to work on these function and try to rewrite them better and more efficient.\n",
    "def make_idx_tr_pairs(indices: Tensor, means: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        indices: a tensor that contains indices of most to least significant landmarks for each frame\n",
    "        means: output of the above function it is basically used as a transparency score from 0 to 10\n",
    "\n",
    "    Returns:\n",
    "        idx_tr: an ordered list containing indices of most to least significant landmarks for each frame along with their transparency score\n",
    "\n",
    "    Note:\n",
    "        the reason idx_tr is an ordered list is so that we can draw landmarks from least to most important. this is usefull\n",
    "        when for example a left hand landmark that is important hovers over a face landmrk. in this case it is drawn on top of \n",
    "        the less imoprtant landmark.\n",
    "    \"\"\"\n",
    "    #prepare transparency_level\n",
    "    transparency_level= means/ torch.max(abs(means))\n",
    "    transparency_level= transparency_level * 10\n",
    "    \n",
    "    \n",
    "    list_1= indices.tolist()\n",
    "    list_2= transparency_level.int().tolist()\n",
    "    \n",
    "    idx_trs = []\n",
    "    for f in range(len(list_1[0])):  \n",
    "        frame = []\n",
    "        for i in range(len(list_1[0][f])):\n",
    "            index = list_1[0][f][i]  \n",
    "            value = list_2[0][f][index]  \n",
    "            frame.append((index, value))  \n",
    "        idx_trs.append(frame)  \n",
    "\n",
    "    return idx_trs\n",
    "\n",
    "\n",
    "def plot_mp_landmarks(frame, result):\n",
    "    \"\"\"\n",
    "    This function draws landmarks and connections on a given frame.\n",
    "    Args:\n",
    "        frame: video frame that we want to draw on.\n",
    "        result: the detected media pipe object corresponding to the frame.\n",
    "        \n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2),\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2))\n",
    "    mp_drawing.draw_landmarks(frame, result.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2),\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2))\n",
    "    mp_drawing.draw_landmarks(frame, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2),\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2))\n",
    "    mp_drawing.draw_landmarks(frame, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2),\n",
    "                              mp_drawing.DrawingSpec(color= (0, 255, 0), thickness= 1, circle_radius= 2))\n",
    "\n",
    "\n",
    "\n",
    "def plot_circle(frame, coor, idx_tr):\n",
    "    \"\"\"\n",
    "    This function visualizes layer attributions in a frame.\n",
    "    Args:\n",
    "        frame: video frame that we want to draw on.\n",
    "        coor: list of all (x, y) coordinates of the landmarks detected in the frame\n",
    "        idx_tr: contain indexes of landmarks and their transparency score. the indexes correspond to the \n",
    "        coordinates of the landmarks in coor list.\n",
    "    Note:\n",
    "        for more information about landmark indexes take a look at get_landmarks and draw_layer_attr functions.\n",
    "        \n",
    "    \"\"\"\n",
    "    for idx, tr in reversed(idx_tr[:75]):\n",
    "        intensity = int(min(255, max(0, 255 * abs(tr) / 10)))\n",
    "\n",
    "        color = (intensity, 255, 0)  \n",
    "\n",
    "        cv2.circle(frame, (coor[idx][0], coor[idx][1]), radius=5, color=color, thickness=-1)\n",
    "\n",
    "\n",
    "def draw_layer_attr(video_path, results, pixel_coor, idx_trs, frame_numbers = 30, wait= 200):\n",
    "    \"\"\"\n",
    "    This function visualizes layer attributions in the video.\n",
    "    Args:\n",
    "        video_path: path to the video.\n",
    "        results: set of all (x, y) coordinates of the landmarks detected in the frame\n",
    "        pixel_coor: this list contains indexes of most important landmarks, it can be any number from 0 to 542.\n",
    "        idx_trs: an ordered list of tuples containing the index landmarks and their transparency\n",
    "        \n",
    "    \"\"\"\n",
    "    #'C:/Users/sadeg/OneDrive/Desktop/Thesis/python_codes/lsa64_raw/all/001_001_001.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"ERROR in opening the video path{video_path}\")\n",
    "    else:\n",
    "        total_frames_number = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_idxs_to_process = np.linspace(0, total_frames_number - 1, frame_numbers, dtype=int)\n",
    "        \n",
    "        for frame_idx, result, coor, idx_tr in zip(frame_idxs_to_process ,results , pixel_coor, idx_trs):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            plot_mp_landmarks(frame, result)\n",
    "            plot_circle(frame, coor, idx_tr)\n",
    "\n",
    "            width = int(frame.shape[1] * 0.60)\n",
    "            height = int(frame.shape[0] * 0.60)\n",
    "            resized_frame = cv2.resize(frame, (width, height))\n",
    "\n",
    "            cv2.imshow(\"Video\", resized_frame)\n",
    "        \n",
    "            # Set wait time to 33 milliseconds for approx. 30 fps\n",
    "            if cv2.waitKey(wait) & 0xFF == 27:  # Exit on ESC key\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba34a1-be16-445a-9e1d-8a60676d4798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
